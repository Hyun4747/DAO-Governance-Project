{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NmaW7XOjsPDN"
      },
      "outputs": [],
      "source": [
        "import selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver import ActionChains\n",
        "\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "from selenium.webdriver import ChromeOptions\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.ui import Select\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "import time\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "from python_graphql_client import GraphqlClient\n",
        "from gql import gql, Client\n",
        "from gql.transport.requests import RequestsHTTPTransport\n",
        "import csv\n",
        "\n",
        "\n",
        "import http.client\n",
        "import shutil\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "from covalent import Client\n",
        "import asyncio\n",
        "import scipy.stats\n",
        "import csv\n",
        "from itertools import islice\n",
        "import chardet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS-YVpxjsPDO"
      },
      "source": [
        "### 1. Using Snapshot API, Download proposal data list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rND-z27BsPDP"
      },
      "outputs": [],
      "source": [
        "# 1) Get available dao list from Boardroom & snapshot_checked\n",
        "# Using Boardroom API : Graphql on the boardrrom website\n",
        "\n",
        "A = pd.read_excel(r\"C:\\Users\\Administrator\\Desktop\\chunghyun\\python\\boardroom_snapshot_added.xlsx\", sheet_name='over_20proposals')\n",
        "snapshot_space_list = A['snapshot_name'].dropna().tolist()\n",
        "snapshot_space_list.remove('-')\n",
        "snapshot_space_list = set(snapshot_space_list)\n",
        "\n",
        "snapshot_space_list= list(snapshot_space_list)\n",
        "snapshot_space_list.sort()\n",
        "print(len(snapshot_space_list))\n",
        "snapshot_space_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viGtrXINsPDW",
        "outputId": "384102e2-c564-468d-82b1-0e8b182030cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nproposal_ID : id \\nnetwork : 1(Ethereum Mainnet), 2(Testnet)\\nvotes : votes count\\n\\n\\n'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2) Using Snapshot API, Download proposal list using grphql\n",
        "client = Client(transport=RequestsHTTPTransport(url=\"https://hub.snapshot.org/graphql\"))\n",
        "\n",
        "query = gql(\"\"\"\n",
        "    query GetProposals($spaceName: String) {\n",
        "        proposals(\n",
        "            first: 1000,\n",
        "            skip: 0,\n",
        "            where: { space_in: [$spaceName], state: \"closed\" },\n",
        "            orderBy: \"created\",\n",
        "            orderDirection: asc\n",
        "        ) {\n",
        "            ...proposalFields\n",
        "        }\n",
        "    }\n",
        "\n",
        "    fragment proposalFields on Proposal {\n",
        "        id\n",
        "        network\n",
        "        title\n",
        "        author\n",
        "        votes\n",
        "        type\n",
        "        choices\n",
        "        scores\n",
        "        scores_total\n",
        "        discussion\n",
        "        link\n",
        "        quorum\n",
        "        start\n",
        "        end\n",
        "        state\n",
        "        space {\n",
        "            id\n",
        "            name\n",
        "        }\n",
        "    }\n",
        "\"\"\")\n",
        "\n",
        "'''\n",
        "proposal_ID : id\n",
        "network : 1(Ethereum Mainnet), 2(Testnet)\n",
        "votes : votes count\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSwGmc4AsPDX"
      },
      "outputs": [],
      "source": [
        "## graphql에서 protocol list 구한 후에 for loop로 proposal csv 만들기 by protocol\n",
        "## add real time column\n",
        "errored_space = []\n",
        "\n",
        "for index, space in enumerate(snapshot_space_list[1:]):\n",
        "    print(f'Processing space {space}, index {index + 1}/{len(snapshot_space_list)}')\n",
        "\n",
        "    try:\n",
        "        result = client.execute(query, variable_values={\"spaceName\": space})\n",
        "        df = pd.DataFrame(result['proposals'])\n",
        "        df.insert(13, 'start_real_time', pd.to_datetime(df['start'], unit='s'))\n",
        "        df.insert(15, 'end_real_time', pd.to_datetime(df['end'], unit='s'))\n",
        "\n",
        "        file_name = f'C:/Users/Administrator/Desktop/chunghyun/python/proposals_snapshot/{space}_proposals_snapshot.csv'\n",
        "        df.to_csv(file_name, index=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing space {space}: {e}\")\n",
        "        errored_space.append(space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sut5j7qPsPDX"
      },
      "source": [
        "### 2. Downloading voting data by protocol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reXFPNe0sPDX"
      },
      "outputs": [],
      "source": [
        "# 3) Download all the voting data available in Snapshot using selenium\n",
        "# 3-1) Download all the voting data available in Snapshot using selenium by protocol\n",
        "\n",
        "directory_path = r'C:\\Users\\Administrator\\Desktop\\chunghyun\\python\\proposals_snapshot'\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(directory_path):\n",
        "    # List all files in the directory\n",
        "    files = os.listdir(directory_path)\n",
        "print(len(files))\n",
        "print(files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52RfKJPjsPDY"
      },
      "outputs": [],
      "source": [
        "# 3-2) Checking tarhet item's index in the list\n",
        "target_item = \"tracer.eth\" +\"_proposals_snapshot.csv\"\n",
        "\n",
        "# Check if the target item is in the list\n",
        "if target_item in files:\n",
        "    # Get the index of the target item and print it\n",
        "    index_of_item = files.index(target_item)\n",
        "    print(f\"The index of {target_item} is: {index_of_item}\")\n",
        "else:\n",
        "    print(f\"{target_item} is not in the list.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FaqFYRIsPDY"
      },
      "outputs": [],
      "source": [
        "# 3-3) Downloading voting data using selenium, Sanpshot downloading one by one\n",
        "\n",
        "for idx, file in enumerate(files[:]):\n",
        "    print(f'Processing space {file}, index {idx+1}/{len(files[146:155])}')\n",
        "    csv_file = f\"C:/Users/Administrator/Desktop/chunghyun/python/proposals_snapshot/{file}\"\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Extracting Proposal ID list\n",
        "    proposal_ID_list = df['id'].tolist()\n",
        "\n",
        "    # URL setting : snapshot.org/#/uniswap/proposal/\"proposal ID\"\n",
        "    URL_list = []\n",
        "    space_name = file.split(\"_\")[0]\n",
        "\n",
        "    base_url = f\"https://snapshot.org/#/{space_name}/proposal/\"\n",
        "    for proposal_id in proposal_ID_list:\n",
        "        url = base_url + proposal_id\n",
        "        URL_list.append(url)\n",
        "\n",
        "    # Set the download directory path\n",
        "    new_folder = f\"{space_name}_voting_data\"\n",
        "    download_dir = r\"C:\\Users\\Administrator\\Desktop\\chunghyun\\python\\voting_data_by_protocol\"\n",
        "    new_directory_path = os.path.join(download_dir, new_folder)\n",
        "    os.makedirs(new_directory_path, exist_ok=True)\n",
        "\n",
        "    # Set the maximum time to wait for an element (in seconds)\n",
        "    implicit_wait_time = 7\n",
        "\n",
        "    # Configure ChromeOptions to set the download directory\n",
        "    options = Options()\n",
        "    prefs = {\"download.default_directory\": new_directory_path}\n",
        "    options.add_experimental_option(\"prefs\", prefs)\n",
        "\n",
        "    # Start a new Chrome browser instance with the configured options\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "\n",
        "    errored_url = []\n",
        "    try:\n",
        "        # Set the implicit wait\n",
        "        driver.implicitly_wait(implicit_wait_time)\n",
        "\n",
        "        for file_idx, url in enumerate(URL_list):\n",
        "            try:\n",
        "                # Open the URL in the browser\n",
        "                driver.get(url)\n",
        "                time.sleep(3)\n",
        "                driver.execute_script(\"window.scrollBy(0, 300);\")\n",
        "                # Find and click the download button\n",
        "\n",
        "                download_button_xpath = '//*[@id=\"content-left\"]/div[3]/div/div[1]/button'\n",
        "                download_button = driver.find_element(By.XPATH, download_button_xpath)\n",
        "\n",
        "                time.sleep(5)\n",
        "                download_button.click()\n",
        "\n",
        "                # Wait for the file to download (you can add an explicit wait here if needed)\n",
        "                # Here, we use a short sleep as a simple wait for demonstration purposes.\n",
        "                time.sleep(10)  # Adjust the wait time as needed\n",
        "\n",
        "                # Get a list of files in the download directory\n",
        "                downloaded_files = os.listdir(new_directory_path)\n",
        "                id = url.split(\"/\")[-1]\n",
        "\n",
        "                # Find the downloaded file name (assuming it is the only file downloaded in the directory)\n",
        "                downloaded_file_name = None\n",
        "                for file_name in downloaded_files:\n",
        "                    if \"snapshot-report-\" + id in file_name:\n",
        "                        downloaded_file_name = file_name\n",
        "                        break\n",
        "\n",
        "                # Check if the expected file name exists\n",
        "                if downloaded_file_name is not None:\n",
        "                    print(f\"File {file_idx+1}/{len(URL_list)} downloaded successfully.\")\n",
        "                else:\n",
        "                    # If the first button fails, try the second button\n",
        "                    try:\n",
        "                        cancel_pop_up_button_xpath ='/html/body/div[2]/div/div[2]/button'\n",
        "                        cancel_pop_up_button = driver.find_element(By.XPATH, cancel_pop_up_button_xpath)\n",
        "                        cancel_pop_up_button.click()\n",
        "\n",
        "                        time.sleep(3)\n",
        "                        driver.execute_script(\"window.scrollBy(0, 300);\")\n",
        "                        time.sleep(8)\n",
        "\n",
        "                        second_download_button_xpath = '/html/body/div[1]/div[1]/div[2]/div[3]/div/div/div/div[1]/div[3]/div[2]/div[1]/button'\n",
        "                        second_download_button = driver.find_element(By.XPATH, second_download_button_xpath)\n",
        "                        time.sleep(5)\n",
        "                        second_download_button.click()\n",
        "\n",
        "                        time.sleep(7)\n",
        "\n",
        "                        downloaded_files = os.listdir(new_directory_path)\n",
        "                        downloaded_file_name = None\n",
        "                        for file_name in downloaded_files:\n",
        "                            if \"snapshot-report-\" + id in file_name:\n",
        "                                downloaded_file_name = file_name\n",
        "                                break\n",
        "\n",
        "                        if downloaded_file_name is not None:\n",
        "                            print(f\"File {file_idx+1}/{len(URL_list)} downloaded successfully after second attempt.\")\n",
        "                        else:\n",
        "                            raise Exception(f\"File download failed for URL: {url}.\")\n",
        "\n",
        "                    except Exception as second_button_error:\n",
        "                        errored_url.append(url)\n",
        "                        print(f\"Error while processing URL: {url} after trying the second button\")\n",
        "\n",
        "            except Exception as e:\n",
        "                errored_url.append(url)\n",
        "                print(f\"Error while processing URL: {url}.\")\n",
        "\n",
        "        print(f\"{space_name}'s errored url len : {len(errored_url)}\")\n",
        "        errored_df = pd.DataFrame({'Errored URL': errored_url})\n",
        "\n",
        "        # Write errored URLs to a CSV file if there are any\n",
        "        if not errored_df.empty:\n",
        "            csv_file_path = f\"C:/Users/Administrator/Desktop/chunghyun/python/errored_urls_by_protocol/{space_name}_errored_urls_{len(errored_url)}.csv\"\n",
        "            errored_df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "            print(f\"Errored URLs written to {csv_file_path}.\")\n",
        "\n",
        "        else :\n",
        "            print(\"No errors while processing URLs.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error : error at opening browser\")\n",
        "\n",
        "    finally:\n",
        "        driver.quit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w2kzsqesPDY"
      },
      "source": [
        "### 3. Errored files retrial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRXv-WAosPDY"
      },
      "outputs": [],
      "source": [
        "csv_folder = r\"C:\\Users\\ChunghyunHan\\Desktop\\Chunghyun Han\"\n",
        "\n",
        "# Get a list of CSV files in the folder\n",
        "csv_files = [file for file in os.listdir(csv_folder) if file.endswith('.csv')]\n",
        "\n",
        "# Specify the folder containing CSV files\n",
        "csv_folder = r\"C:\\Users\\ChunghyunHan\\Desktop\\Chunghyun Han\"\n",
        "\n",
        "# Get a list of CSV files in the folder\n",
        "csv_files = [file for file in os.listdir(csv_folder) if file.endswith('.csv')]\n",
        "\n",
        "# 9:13\n",
        "for file in csv_files[39:]:\n",
        "    filename = file.split('_')[0]\n",
        "    print(f\"Processing file: {filename}\")\n",
        "\n",
        "    # Read the CSV file and extract the 'errored_url' column\n",
        "    csv_path = os.path.join(csv_folder, file)\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    if 'Errored URL' not in df.columns:\n",
        "        print(\"No 'Errored URL' column found in the file. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    errored_url_list = df['Errored URL'].tolist()\n",
        "    errored_url_list.sort()\n",
        "\n",
        "\n",
        "    # Set the download directory path\n",
        "    new_folder = f\"{filename}_errored_voting_data_trial2\"\n",
        "    download_dir = r\"C:\\Users\\ChunghyunHan\\Desktop\\Chunghyun Han\\errorred_urls_by_protocol\"\n",
        "    new_directory_path = os.path.join(download_dir, new_folder)\n",
        "    os.makedirs(new_directory_path, exist_ok=True)\n",
        "\n",
        "    # Set the maximum time to wait for an element (in seconds)\n",
        "    implicit_wait_time = 7\n",
        "\n",
        "    # Configure ChromeOptions to set the download directory\n",
        "    options = Options()\n",
        "    prefs = {\"download.default_directory\": new_directory_path}\n",
        "    options.add_experimental_option(\"prefs\", prefs)\n",
        "\n",
        "    # Start a new Chrome browser instance with the configured options\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "\n",
        "    errored_url_after_trial2 = []\n",
        "\n",
        "    try:\n",
        "        # Set the implicit wait\n",
        "        driver.implicitly_wait(implicit_wait_time)\n",
        "        #\n",
        "        for file_idx, url in enumerate(errored_url_list[:]):\n",
        "            try:\n",
        "                # Open the URL in the browser\n",
        "                driver.get(url)\n",
        "\n",
        "                # Find and click the download button\n",
        "                download_button_xpath = '//*[@id=\"content-left\"]/div[3]/div/div[1]/button'\n",
        "                download_button = driver.find_element(By.XPATH, download_button_xpath)\n",
        "                time.sleep(1)\n",
        "                download_button.click()\n",
        "\n",
        "                # Wait for the file to download (you can add an explicit wait here if needed)\n",
        "                # Here, we use a short sleep as a simple wait for demonstration purposes.\n",
        "                time.sleep(5)  # Adjust the wait time as needed\n",
        "\n",
        "                # Get a list of files in the download directory\n",
        "                downloaded_files = os.listdir(new_directory_path)\n",
        "                id = url.split(\"/\")[-1]\n",
        "\n",
        "                # Find the downloaded file name (assuming it is the only file downloaded in the directory)\n",
        "                downloaded_file_name = None\n",
        "                for file_name in downloaded_files:\n",
        "                    if \"snapshot-report-\" + id in file_name:\n",
        "                        downloaded_file_name = file_name\n",
        "                        break\n",
        "\n",
        "                # Check if the expected file name exists\n",
        "                if downloaded_file_name is not None:\n",
        "                    print(f\"File {file_idx+1}/{len(errored_url_list)} downloaded successfully.\")\n",
        "                else:\n",
        "                    # If the first button fails, try the second button\n",
        "                    try:\n",
        "                        cancel_pop_up_button_xpath ='/html/body/div[2]/div/div[2]/button'\n",
        "                        cancel_pop_up_button = driver.find_element(By.XPATH, cancel_pop_up_button_xpath)\n",
        "                        cancel_pop_up_button.click()\n",
        "\n",
        "                        time.sleep(2)\n",
        "                        driver.execute_script(\"window.scrollBy(0, 500);\")\n",
        "                        time.sleep(3)\n",
        "\n",
        "                        second_download_button_xpath = '/html/body/div[1]/div[1]/div[2]/div[3]/div/div/div/div[1]/div[3]/div[2]/div[1]/button'\n",
        "                        second_download_button = driver.find_element(By.XPATH, second_download_button_xpath)\n",
        "                        time.sleep(5)\n",
        "                        second_download_button.click()\n",
        "\n",
        "                        time.sleep(5)\n",
        "\n",
        "                        downloaded_files = os.listdir(new_directory_path)\n",
        "                        downloaded_file_name = None\n",
        "                        for file_name in downloaded_files:\n",
        "                            if \"snapshot-report-\" + id in file_name:\n",
        "                                downloaded_file_name = file_name\n",
        "                                break\n",
        "\n",
        "                        if downloaded_file_name is not None:\n",
        "                            print(f\"File {file_idx+1}/{len(errored_url_list)} downloaded successfully after second attempt.\")\n",
        "                        else:\n",
        "                            raise Exception(f\"File download failed for URL: {url}.\")\n",
        "\n",
        "                    except Exception as second_button_error:\n",
        "                        errored_url_after_trial2.append(url)\n",
        "                        print(f\"Error while processing URL: {url} after trying the second button\")\n",
        "\n",
        "            except Exception as e:\n",
        "                errored_url_after_trial2.append(url)\n",
        "                print(f\"Error while processing URL: {url}.\")\n",
        "\n",
        "        print(f\"{filename}'s errored url len : {len(errored_url_after_trial2)}\")\n",
        "        errored_df = pd.DataFrame({'Errored URL': errored_url_after_trial2})\n",
        "\n",
        "        # Write errored URLs to a CSV file if there are any\n",
        "        if not errored_df.empty:\n",
        "            csv_file_path = f\"C:/Users/ChunghyunHan/Desktop/Chunghyun Han/errorred_urls_trial2/{filename}_errored_urls_{len(errored_url_after_trial2)}_trial2.csv\"\n",
        "            errored_df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "            print(f\"Errored URLs after trial 2 written to {csv_file_path}.\")\n",
        "\n",
        "        else :\n",
        "            print(\"No errors while processing URLs.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error : error at opening browser\")\n",
        "\n",
        "    finally:\n",
        "        driver.quit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gULqVsocsPDZ"
      },
      "outputs": [],
      "source": [
        "df_retrial = pd.read_csv(r\"C:\\Users\\ChunghyunHan\\Desktop\\Chunghyun Han\\errorred_urls_trial2\\aavegotchi.eth_errored_urls_61_trial2.csv\")\n",
        "gotchi_retrial = df_retrial['Errored URL'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZcxf547sPDZ"
      },
      "outputs": [],
      "source": [
        "## files matching test\n",
        "\n",
        "df_retrial_aave = pd.read_csv(r\"C:\\Users\\ChunghyunHan\\Desktop\\Chunghyun Han\\gnosis.eth_errored_urls_72.csv\")\n",
        "errored_list = df_retrial_aave['Errored URL'].tolist()\n",
        "\n",
        "errored_trial2 = [url.split(\"/\")[-1] for url in errored_list]\n",
        "print(len(errored_trial2))\n",
        "print(errored_trial2)\n",
        "\n",
        "directory_path = r\"C:\\Users\\ChunghyunHan\\Desktop\\Chunghyun Han\\errorred_urls_by_protocol\\gnosis.eth_errored_voting_data_trial2\"\n",
        "\n",
        "# Get a list of files in the directory\n",
        "files_list = os.listdir(directory_path)\n",
        "downloaded_proposal_list = [file.split('-')[-1].replace('.csv', '') for file in files_list]\n",
        "\n",
        "set_errored_aave_trial2 = set(errored_trial2)\n",
        "set_downloaded_proposal_list = set(downloaded_proposal_list)\n",
        "\n",
        "# Find missing elements in the downloaded_proposal_list compared to errored_aave_trial2\n",
        "missing_elements = set_errored_aave_trial2 - set_downloaded_proposal_list\n",
        "print(len(missing_elements))\n",
        "\n",
        "print(\"Missing Elements:\")\n",
        "for element in missing_elements:\n",
        "    print(element)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJR3Pw2PsPDZ"
      },
      "source": [
        "### 4. Boardroom DATA Fetching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# boardroom protocols list\n",
        "df0 = pd.read_excel(r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\boardroom_snapshot_added_1124.xlsx\", sheet_name='over_20proposals')\n",
        "boardroom_list = df0['cname'].dropna().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to fetch protocol information for a given protocol\n",
        "def fetch_protocol_data(protocol):\n",
        "    conn = http.client.HTTPSConnection(\"api.boardroom.info\")\n",
        "    headers = {'Accept': \"application/json\"}\n",
        "\n",
        "    # Concatenate the API key and protocol to the end of the URL\n",
        "    url = f\"/v1/protocols/{protocol}?excludeTokenInfo=false&includeContractMetadata=true&limit=3500&status=closed&key={api_key}\"\n",
        "\n",
        "    conn.request(\"GET\", url, headers=headers)\n",
        "    res = conn.getresponse()\n",
        "    return json.loads(res.read())['data']\n",
        "\n",
        "api_key = '6e171f4a120b41da20910cc813ddf6a0'\n",
        "# https://api.boardroom.info/v1/protocols/{cname}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "protocol_data_list = []\n",
        "\n",
        "for protocol in boardroom_list:\n",
        "    protocol_data = fetch_protocol_data(protocol)\n",
        "    protocol_data_list.append(protocol_data)\n",
        "\n",
        "protocols_df = pd.DataFrame(protocol_data_list)\n",
        "\n",
        "protocols_df.head()\n",
        "protocols_df.to_csv(r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\boardroom_protocols_data.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "190\n",
            "['stargatefi', 'streamreth']\n"
          ]
        }
      ],
      "source": [
        "boardroom_protocols = protocols_df['cname'].tolist()\n",
        "boardroom_protocols.sort()\n",
        "print(len(boardroom_protocols))\n",
        "print(boardroom_protocols[166:168])\n",
        "# error : 'magicappstoreeth', stargatefi "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "lv7YG03esPEq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Snapshot data for streamreth saved, index 1/190\n",
            "Snapshot data for sushi saved, index 2/190\n",
            "Snapshot data for thedreamdaoeth saved, index 3/190\n",
            "Snapshot data for thelanddaopropeth saved, index 4/190\n",
            "Snapshot data for threshold saved, index 5/190\n",
            "Snapshot data for tigervcdaoeth saved, index 6/190\n",
            "Snapshot data for token-engineering-commons saved, index 7/190\n",
            "Snapshot data for tokenlon saved, index 8/190\n",
            "Snapshot data for tornadocash saved, index 9/190\n",
            "Snapshot data for tracereth saved, index 10/190\n",
            "Snapshot data for truefigov saved, index 11/190\n",
            "Snapshot data for ultradaov2 saved, index 12/190\n",
            "Snapshot data for uniswap saved, index 13/190\n",
            "Snapshot data for unlock saved, index 14/190\n",
            "Snapshot data for vareneth saved, index 15/190\n",
            "Snapshot data for voteairswapeth saved, index 16/190\n",
            "Snapshot data for votevitadaoeth saved, index 17/190\n",
            "Snapshot data for vsp saved, index 18/190\n",
            "Snapshot data for wagdieeth saved, index 19/190\n",
            "Snapshot data for wagumi saved, index 20/190\n",
            "Snapshot data for xdaistakeeth saved, index 21/190\n",
            "Snapshot data for yam saved, index 22/190\n",
            "Snapshot data for ybaby saved, index 23/190\n"
          ]
        }
      ],
      "source": [
        "conn = http.client.HTTPSConnection(\"api.boardroom.info\")\n",
        "\n",
        "def fetch_snapshot_data(protocol, limit=3500):\n",
        "    headers = {'Accept': \"application/json\"}\n",
        "\n",
        "    # Concatenate the API key, protocol, limit, and other parameters to the end of the URL\n",
        "    url = f\"/v1/protocols/{protocol}/proposals?orderByIndexedAt=asc&limit={limit}&key={api_key}\"\n",
        "\n",
        "    conn.request(\"GET\", url, headers=headers)\n",
        "    res = conn.getresponse()\n",
        "    return json.loads(res.read())['data']\n",
        "\n",
        "# Directory to save CSV files\n",
        "save_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\Boardroom&Snapshot_data\\Boardroom_proposal_data\"\n",
        "\n",
        "# Function to save DataFrame to CSV file\n",
        "def save_to_csv(df, protocol, directory):\n",
        "    file_path = f\"{directory}\\\\{protocol}_proposal_data.csv\"\n",
        "    df.to_csv(file_path, index=False)\n",
        "    \n",
        "\n",
        "# Loop through each protocol in boardroom_protocols\n",
        "for idx, protocol in enumerate(boardroom_protocols[167:]):\n",
        "    snapshot_data = fetch_snapshot_data(protocol)\n",
        "    \n",
        "    # Create a DataFrame from the snapshot data\n",
        "    df_snapshot = pd.DataFrame(snapshot_data)\n",
        "    df_snapshot = df_snapshot.sort_values(by='startTimestamp')\n",
        "    # Save the DataFrame to a CSV file using the function\n",
        "    save_to_csv(df_snapshot, protocol, save_directory)\n",
        "    print(f\"Snapshot data for {protocol} saved, index {idx+1}/{len(boardroom_protocols)}\")\n",
        "\n",
        "# Close the connection\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG7ES_1CsPEq"
      },
      "source": [
        "### 5. Merged_data Building for each protocol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Win rate for each proposal of all protocols \n",
        "\n",
        "    # check the columns are identical for the protocols\n",
        "directory_path = r'C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\proposals_snapshot_valid'\n",
        "\n",
        "# Ideal column list\n",
        "ideal_columns = ['id', 'network', 'title', 'author', 'votes', 'type', 'choices', 'scores', 'scores_total', 'discussion', 'link', 'quorum', 'start', 'start_real_time', 'end', 'end_real_time', 'state', 'space']\n",
        "\n",
        "# Function to check if columns match the ideal list\n",
        "def check_columns(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        result = chardet.detect(f.read())\n",
        "    encoding = result['encoding']\n",
        "\n",
        "    try:\n",
        "        # Read the CSV file with detected encoding\n",
        "        df = pd.read_csv(file_path, encoding=encoding)\n",
        "        \n",
        "        # Get the columns of the DataFrame\n",
        "        columns = list(df.columns)\n",
        "\n",
        "        # Check if columns match the ideal list\n",
        "        if columns != ideal_columns:\n",
        "            print(f\"Columns in {os.path.basename(file_path)} do not match the ideal list.\")\n",
        "            print(f\"Ideal columns: {ideal_columns}\")\n",
        "            print(f\"Actual columns: {columns}\")\n",
        "            print()\n",
        "\n",
        "        else :\n",
        "            print(f\"Columns in {os.path.basename(file_path)} match the ideal list.\")\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"UnicodeDecodeError: Unable to read file {os.path.basename(file_path)} with detected encoding '{encoding}'. Trying with 'utf-8'.\")\n",
        "        \n",
        "        # Retry reading the CSV file with 'utf-8' encoding\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
        "            \n",
        "            # Get the columns of the DataFrame\n",
        "            columns = list(df.columns)\n",
        "\n",
        "            # Check if columns match the ideal list\n",
        "            if columns != ideal_columns:\n",
        "                print(f\"Columns in {os.path.basename(file_path)} do not match the ideal list.111111\")\n",
        "                print(f\"Ideal columns: {ideal_columns}\")\n",
        "                print(f\"Actual columns: {columns}\")\n",
        "                print()\n",
        "        except pd.errors.EmptyDataError:\n",
        "            print(f\"File {os.path.basename(file_path)} is empty.\")\n",
        "            print()\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"Unable to read file {os.path.basename(file_path)} even with 'utf-8' encoding.\")\n",
        "            print()\n",
        "\n",
        "# Iterate through files in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    file_path = os.path.join(directory_path, filename)\n",
        "    \n",
        "    # Check only for CSV files\n",
        "    if filename.endswith(\".csv\"):\n",
        "        check_columns(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Win rate for each proposal of all protocols\n",
        "# Directory containing the files\n",
        "directory_path = r'C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\proposals_snapshot_valid'\n",
        "\n",
        "# Output directory for modified files\n",
        "output_directory = r'C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\proposals_snapshot_col_adds'\n",
        "\n",
        "# Function to process each file\n",
        "def process_file(file_path):\n",
        "    try:\n",
        "        # Read the CSV file into a DataFrame\n",
        "        df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "\n",
        "        # Check if 'scores_total' is 0 and drop the corresponding rows\n",
        "        df = df[df['scores_total'] != 0]\n",
        "\n",
        "        # Calculate additional columns\n",
        "        df['Num_choices'] = df['choices'].apply(lambda x: len(eval(x)) if x else 0)\n",
        "        df['Win_choice'] = df.apply(lambda x: eval(x['choices'])[eval(x['scores']).index(max(eval(x['scores'])))] if x['choices'] and x['scores'] else None, axis=1)\n",
        "        df['Win_score'] = df['scores'].apply(lambda x: max(eval(x)) if x else None)\n",
        "        df['Winrate'] = df.apply(lambda row: row['Win_score'] / sum(eval(row['scores'])) if row['scores'] else None, axis=1)\n",
        "\n",
        "        # Reorder columns to place the new ones between 'scores' and 'scores_total'\n",
        "        new_columns = ['Num_choices', 'Win_choice', 'Win_score', 'Winrate']\n",
        "        position_to_insert = df.columns.get_loc('scores_total')  # Get the position of 'scores_total'\n",
        "        df = pd.concat([df.iloc[:, :position_to_insert], df[new_columns], df.iloc[:, position_to_insert:]], axis=1)\n",
        "\n",
        "        df = df.iloc[:, :-4]\n",
        "        \n",
        "        print(df.head())\n",
        "\n",
        "        # Construct the output file name\n",
        "        base_filename, file_extension = os.path.splitext(os.path.basename(file_path))\n",
        "        output_filename = f\"{base_filename}_cols_added{file_extension}\"\n",
        "\n",
        "        # Save the modified DataFrame to a new CSV file\n",
        "        output_file_path = os.path.join(output_directory, output_filename)\n",
        "        df.to_csv(output_file_path, index=False)\n",
        "\n",
        "        print(f\"Success: File {output_filename} processed and saved.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {os.path.basename(file_path)}: {e}\")\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Iterate through files in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    print(filename)\n",
        "    file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "    # Check only for CSV files\n",
        "    if filename.endswith(\".csv\"):\n",
        "        process_file(file_path)\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify the main directory containing subdirectories with CSV files\n",
        "main_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\voting_data_by_protocol_master\"\n",
        "\n",
        "# Get a list of subdirectories (each subdirectory represents a protocol)\n",
        "subdirectories = [subdir for subdir in os.listdir(main_directory) if os.path.isdir(os.path.join(main_directory, subdir))]\n",
        "\n",
        "# Iterate through each subdirectory (protocol)\n",
        "for subdir in subdirectories[1:]:\n",
        "    # Construct the full path to the subdirectory\n",
        "    subdirectory_path = os.path.join(main_directory, subdir)\n",
        "\n",
        "    # Get a list of CSV files in the subdirectory\n",
        "    csv_files = [file for file in os.listdir(subdirectory_path) if file.endswith('.csv')]\n",
        "\n",
        "    # Create an empty list to store errored CSV files\n",
        "    errored_files = []\n",
        "\n",
        "    # Iterate through each CSV file in the subdirectory\n",
        "    for file in csv_files:\n",
        "        # Extract proposal_id from the file name\n",
        "        proposal_id = file.split('-')[2].split('.')[0]\n",
        "\n",
        "        # Construct the full file path\n",
        "        file_path = os.path.join(subdirectory_path, file)\n",
        "\n",
        "        try:\n",
        "            # Read the CSV file into a DataFrame\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Rename the 'address' column to 'voter_address'\n",
        "            df.rename(columns={'address': 'voter_address'}, inplace=True)\n",
        "\n",
        "            # Insert the 'proposal_id' column as the first column\n",
        "            df.insert(0, 'proposal_id', proposal_id)\n",
        "\n",
        "            # Save the modified DataFrame back to the CSV file\n",
        "            df.to_csv(file_path, index=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Append the name of the errored CSV file to the list\n",
        "            errored_files.append(file)\n",
        "            print(f\"Error processing file '{file}': {e}\")\n",
        "\n",
        "    # Create a subdirectory for errored files within the current protocol's directory\n",
        "    errored_files_subdir = os.path.join(subdirectory_path, 'errored_files')\n",
        "    os.makedirs(errored_files_subdir, exist_ok=True)\n",
        "\n",
        "    # Move the errored CSV files into the 'errored_files' subdirectory\n",
        "    for errored_file in errored_files:\n",
        "        source_path = os.path.join(subdirectory_path, errored_file)\n",
        "        dest_path = os.path.join(errored_files_subdir, errored_file)\n",
        "        shutil.move(source_path, dest_path)\n",
        "\n",
        "    # Print a message indicating the completion of processing for the current subdirectory\n",
        "    print(f\"Processed folder: {subdir}\")\n",
        "\n",
        "    # Print errored files for the current subdirectory\n",
        "    if errored_files:\n",
        "        print(f\"Errored files in {subdir}: {errored_files}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## RETRIAL \n",
        "'''\n",
        "main_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\voting_data_by_protocol_master\"\n",
        "\n",
        "# Get a list of subdirectories (each subdirectory represents a protocol)\n",
        "subdirectories = [subdir for subdir in os.listdir(main_directory) if os.path.isdir(os.path.join(main_directory, subdir))]\n",
        "\n",
        "# Iterate through each subdirectory (protocol)\n",
        "for subdir in subdirectories[1:]:\n",
        "    # Construct the full path to the subdirectory\n",
        "    subdirectory_path = os.path.join(main_directory, subdir)\n",
        "\n",
        "    # Get a list of CSV files in the 'errored_files' subdirectory\n",
        "    errored_files_subdir = os.path.join(subdirectory_path, 'errored_files')\n",
        "    csv_files = [file for file in os.listdir(errored_files_subdir) if file.endswith('.csv')]\n",
        "\n",
        "    # Create an empty list to store errored CSV files\n",
        "    errored_files = []\n",
        "\n",
        "    # Iterate through each CSV file in the 'errored_files' subdirectory\n",
        "    for file in csv_files:\n",
        "        # Extract proposal_id from the file name\n",
        "        proposal_id = file.split('-')[2].split('.')[0]\n",
        "\n",
        "        # Construct the full file path\n",
        "        file_path = os.path.join(errored_files_subdir, file)\n",
        "\n",
        "        try:\n",
        "            # Read the CSV file into a DataFrame\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Rename the 'address' column to 'voter_address'\n",
        "            df.rename(columns={'address': 'voter_address'}, inplace=True)\n",
        "\n",
        "            # Insert the 'proposal_id' column as the first column\n",
        "            df.insert(0, 'proposal_id', proposal_id)\n",
        "\n",
        "            # Save the modified DataFrame back to the CSV file\n",
        "            df.to_csv(file_path, index=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Append the name of the errored CSV file to the list\n",
        "            errored_files.append(file)\n",
        "            print(f\"Error processing file '{file}' in {subdir}: {e}\")\n",
        "\n",
        "    # Print a message indicating the completion of processing for the current subdirectory\n",
        "    print(f\"Processed folder: {subdir}\")\n",
        "\n",
        "    # Print errored files for the current subdirectory\n",
        "    if errored_files:\n",
        "        print(f\"Errored files in {subdir}: {errored_files}\")\n",
        "'''\n",
        "# checking\n",
        "'''\n",
        "main_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\voting_data_by_protocol_master\"\n",
        "\n",
        "# Get a list of subdirectories (each subdirectory represents a protocol)\n",
        "subdirectories = [subdir for subdir in os.listdir(main_directory) if os.path.isdir(os.path.join(main_directory, subdir))]\n",
        "\n",
        "# Check if all 'errored_files' folders are empty\n",
        "all_errored_files_empty = True\n",
        "\n",
        "# Iterate through each subdirectory (protocol)\n",
        "for subdir in subdirectories[1:]:\n",
        "    # Construct the full path to the 'errored_files' subdirectory\n",
        "    errored_files_subdir = os.path.join(main_directory, subdir, 'errored_files')\n",
        "\n",
        "    # Check if the 'errored_files' subdirectory is empty\n",
        "    if os.listdir(errored_files_subdir):\n",
        "        all_errored_files_empty = False\n",
        "        print(f\"The 'errored_files' folder in {subdir} is not empty.\")\n",
        "\n",
        "# Print the result\n",
        "if all_errored_files_empty:\n",
        "    print(\"All 'errored_files' folders are empty.\")\n",
        "else:\n",
        "    print(\"Not all 'errored_files' folders are empty.\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # keep only necessary columns\n",
        "# # Specify the main directory containing subdirectories with CSV files\n",
        "# main_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\voting_data_by_protocol_master\"\n",
        "\n",
        "# # Get a list of subdirectories (each subdirectory represents a protocol)\n",
        "# subdirectories = [subdir for subdir in os.listdir(main_directory) if os.path.isdir(os.path.join(main_directory, subdir))]\n",
        "\n",
        "# # Iterate through each subdirectory (protocol)\n",
        "# for subdir in subdirectories:\n",
        "#     # Construct the full path to the subdirectory\n",
        "#     subdirectory_path = os.path.join(main_directory, subdir)\n",
        "\n",
        "#     # Get a list of CSV files in the subdirectory\n",
        "#     csv_files = [file for file in os.listdir(subdirectory_path) if file.endswith('.csv')]\n",
        "\n",
        "#     # Iterate through each CSV file in the subdirectory\n",
        "#     for file in csv_files:\n",
        "#         # Construct the full file path\n",
        "#         file_path = os.path.join(subdirectory_path, file)\n",
        "\n",
        "#         # Read the CSV file into a DataFrame\n",
        "#         df = pd.read_csv(file_path)\n",
        "\n",
        "#         # Check if the last column is 'timestamp'\n",
        "#         if df.columns[-1] == 'timestamp':\n",
        "#             # No further action needed if the last column is already 'timestamp'\n",
        "#             print(f\"File '{file}' already conforms to the desired format.\")\n",
        "#         else:\n",
        "#             # Drop columns after 'timestamp' and save the modified DataFrame\n",
        "#             df = df.loc[:, :'timestamp']\n",
        "#             df.to_csv(file_path, index=False)\n",
        "#             print(f\"Modified file '{file}' to ensure the last column is 'timestamp'.\")\n",
        "\n",
        "#     print(f\"Processing completed for subdirectory: {subdir}\")\n",
        "\n",
        "# print(\"All processing completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# format uniformity\n",
        "# Specify the main directory containing subdirectories with CSV files\n",
        "main_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\voting_data_by_protocol_master\"\n",
        "\n",
        "# Get a list of subdirectories (each subdirectory represents a protocol)\n",
        "subdirectories = [subdir for subdir in os.listdir(main_directory) if os.path.isdir(os.path.join(main_directory, subdir))]\n",
        "\n",
        "# Iterate through each subdirectory (protocol)\n",
        "for subdir in subdirectories:\n",
        "    # Construct the full path to the subdirectory\n",
        "    subdirectory_path = os.path.join(main_directory, subdir)\n",
        "\n",
        "    # Get a list of CSV files in the subdirectory\n",
        "    csv_files = [file for file in os.listdir(subdirectory_path) if file.endswith('.csv')]\n",
        "\n",
        "    # Initialize the count of errored files\n",
        "    errored_files_count = 0\n",
        "\n",
        "    # Check if there are CSV files with columns starting with 'choice.'\n",
        "    if any(any(col.startswith('choice.') for col in pd.read_csv(os.path.join(subdirectory_path, file), nrows=0).columns) for file in csv_files):\n",
        "        # Create a new folder 'multiple_choices' in the current subdirectory\n",
        "        multiple_choices_folder = os.path.join(subdirectory_path, 'multiple_choices')\n",
        "        os.makedirs(multiple_choices_folder, exist_ok=True)\n",
        "\n",
        "        # Iterate through each CSV file in the subdirectory\n",
        "        for file in csv_files:\n",
        "            # Construct the full file path\n",
        "            file_path = os.path.join(subdirectory_path, file)\n",
        "\n",
        "            try:\n",
        "                # Read the CSV file into a DataFrame\n",
        "                df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "                \n",
        "                # Check if the file has multiple choice columns\n",
        "                if any(col.startswith('choice.') for col in df.columns):\n",
        "                    # Move the original file to the 'multiple_choices' folder\n",
        "                    shutil.move(file_path, os.path.join(multiple_choices_folder, file))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file '{file}': {e}\")\n",
        "                errored_files_count += 1\n",
        "                # Create a new folder 'errored_files' in the current subdirectory\n",
        "                errored_files_folder = os.path.join(subdirectory_path, 'errored_files')\n",
        "                os.makedirs(errored_files_folder, exist_ok=True)\n",
        "                # Move the errored file to the 'errored_files' folder\n",
        "                shutil.move(file_path, os.path.join(errored_files_folder, file))\n",
        "\n",
        "        print(f\"Processed folder: {subdir} - 'multiple_choices' folder created.\")\n",
        "        print(f\"Number of errored files: {errored_files_count}\")\n",
        "    else:\n",
        "        print(f\"No files with 'choice.' columns found in folder: {subdir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## adding proposal_id for all protocols\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "main_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\voting_data_by_protocol_master\"\n",
        "\n",
        "errored_files = []\n",
        "# Iterate through subdirectories\n",
        "for subdir, _, files in os.walk(main_directory):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(subdir, file)\n",
        "        \n",
        "        try:\n",
        "            # Read the CSV file into a DataFrame\n",
        "            df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "\n",
        "            # Check if 'proposal_id' column already exists\n",
        "            if 'proposal_id' not in df.columns:\n",
        "                # Extract proposal_id from the filename\n",
        "                proposal_id = file.split('-')[2].split('.')[0]\n",
        "                \n",
        "                # Add 'proposal_id' column with the extracted proposal_id\n",
        "                df.insert(0, 'proposal_id', proposal_id)\n",
        "\n",
        "                # Save the modified DataFrame back to the original file\n",
        "                df.to_csv(file_path, index=False)\n",
        "                print(f\"Added 'proposal_id' column to file: {file} in directory: {subdir}\")\n",
        "            else:\n",
        "                # print(f\"'proposal_id' column already exists in file: {file} in directory: {subdir}\")\n",
        "                pass\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file '{file}' in directory: {subdir} - {e}\")\n",
        "            errored_files.append(file_path)\n",
        "\n",
        "print(f\"Number of errored files: {len(errored_files)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### merged files building\n",
        "1) preprocess : make the form uniformly, and concat (stacked), drop 'reason' column\n",
        "2) merge inner, with cols_added file and boardroom contents file\n",
        "3) merge 2) file to the 'xxx_concat file' \n",
        "    (1) cols_added file column rename : id -> proposal_id\n",
        "    (2) cols_added file column 'network', 'space' drop\n",
        "    (3) _concat file column 'author_ipfs_hash' drop\n",
        "    (4) _concat file 'choice' col is nan -> drop the line\n",
        "\n",
        "\n",
        "FT, NFT seperately done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# preprocessing : building concat files\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "main_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\voting_data_by_protocol_master\"\n",
        "\n",
        "# Specify the directory to save merged files\n",
        "output_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\merged_data_files\\preprocessing_voting_concat\"\n",
        "\n",
        "# Get a list of protocol folders\n",
        "protocol_folders = [folder for folder in os.listdir(main_directory) if os.path.isdir(os.path.join(main_directory, folder))]\n",
        "\n",
        "for protocol_folder in protocol_folders[70:]:\n",
        "    print(f\"Processing protocol folder: {protocol_folder}\")\n",
        "    # Construct the full path to the protocol folder\n",
        "    protocol_folder_path = os.path.join(main_directory, protocol_folder)\n",
        "\n",
        "    # Get a list of CSV files in the protocol folder\n",
        "    csv_files = [file for file in os.listdir(protocol_folder_path) if file.endswith('.csv')]\n",
        "\n",
        "    # Create an empty list to store DataFrames\n",
        "    dfs = []\n",
        "\n",
        "    # Iterate through each CSV file in the protocol folder\n",
        "    for csv_file in csv_files:\n",
        "        # Construct the full file path\n",
        "        file_path = os.path.join(protocol_folder_path, csv_file)\n",
        "\n",
        "        try:\n",
        "            # Read the CSV file into a DataFrame\n",
        "            df = pd.read_csv(file_path, low_memory=False)\n",
        "\n",
        "            # Drop the 'reason' column\n",
        "            df = df.drop('author_ipfs_hash', axis=1, errors='ignore')\n",
        "            df = df.drop('reason', axis=1, errors='ignore')\n",
        "\n",
        "            # Append the DataFrame to the list\n",
        "            dfs.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in file: {file_path}. Error: {e}\")\n",
        "            \n",
        "    # Concatenate all DataFrames into one vertically\n",
        "    concatenated_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
        "\n",
        "    # Save the concatenated DataFrame to a new CSV file\n",
        "    output_file_path = os.path.join(output_directory, f\"{protocol_folder}_concat.csv\")\n",
        "    concatenated_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "    print(f\"Processed protocol folder: {protocol_folder}\")\n",
        "\n",
        "print(\"Merging and preprocessing completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cream-finance\n",
            "Merged and saved file for protocol: cream-finance\n"
          ]
        }
      ],
      "source": [
        "## Merge 1 : cols_added+boardroom data\n",
        "file1_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\merged_data_files\\proposals_snapshot_col_adds\"\n",
        "file2_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\Boardroom&Snapshot_data\\Boardroom_proposal_data_contents\"\n",
        "\n",
        "# Specify the output directory for merged files\n",
        "output_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\merged_data_files\\preprocessing111\"\n",
        "\n",
        "def extract_columns(file_path):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)\n",
        "    protocol_name = file_path.split('_')[0]\n",
        "    df.rename(columns={'id': 'proposal_id'}, inplace=True)\n",
        "    df['proposal_id'] = df['proposal_id'].astype(str).str.lower().str.strip()\n",
        "    return df[['proposal_id', 'blockNumber']], protocol_name\n",
        "\n",
        "# Get the list of files in both directories\n",
        "files_dir1 = [f for f in os.listdir(file1_directory) if f.endswith('.csv')] #cols_added\n",
        "files_dir2 = [f for f in os.listdir(file2_directory) if f.endswith('.csv')] #contents \n",
        "# merge 2 to 1\n",
        "\n",
        "# Iterate through files in directory 1\n",
        "for file1 in files_dir1[32:33]:\n",
        "    # Extract protocol name from file name\n",
        "    protocol_name = file1.split('_')[0].split('.')[0]\n",
        "    print(protocol_name)\n",
        "    \n",
        "    # Find matching file in directory 2\n",
        "    matching_file_in_dir2 = next((f for f in files_dir2 if f == f\"{protocol_name}_proposal_data.csv\"), None)\n",
        "\n",
        "    if matching_file_in_dir2:\n",
        "        # Extract relevant columns from directory 2\n",
        "        df_dir2, protocol_name_dir2 = extract_columns(os.path.join(file2_directory, matching_file_in_dir2))\n",
        "\n",
        "        # Read and extract relevant columns from directory 1\n",
        "        df_dir1 = pd.read_csv(os.path.join(file1_directory, file1), encoding='ISO-8859-1', low_memory=False)\n",
        "        df_dir1.rename(columns={'id': 'proposal_id'}, inplace=True)\n",
        "        df_dir1['proposal_id'] = df_dir1['proposal_id'].astype(str).str.lower().str.strip()\n",
        "        df_dir1 = df_dir1.drop(['network', 'space'], axis=1, errors='ignore')\n",
        "\n",
        "        df_dir2['proposal_id'] = df_dir2['proposal_id'].astype(str).str.lower().str.strip()\n",
        "        df_merged = pd.merge(df_dir1, df_dir2, on='proposal_id', how='inner')\n",
        "\n",
        "        # Save the merged DataFrame to a new CSV file in the output directory\n",
        "        output_file_path = os.path.join(output_directory, f\"{protocol_name}.eth_preprocessing_intersected.csv\")\n",
        "        df_merged.to_csv(output_file_path, index=False)\n",
        "\n",
        "        print(f\"Merged and saved file for protocol: {protocol_name}\")\n",
        "    else:\n",
        "        print(f\"No matching file found for {file1} in directory 2.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged and saved file for protocol: brightmoments.eth\n",
            "Merged and saved file for protocol: bullsontheblock.eth\n",
            "Merged and saved file for protocol: buzzedbears.eth\n",
            "Merged and saved file for protocol: citydao.eth\n",
            "Merged and saved file for protocol: doodles.eth\n",
            "Merged and saved file for protocol: evmavericks.eth\n",
            "Merged and saved file for protocol: expansiondao.eth\n",
            "Merged and saved file for protocol: fatcatsdao.eth\n",
            "Merged and saved file for protocol: futera.eth\n",
            "Merged and saved file for protocol: gmdao.eth\n",
            "Merged and saved file for protocol: gnars.eth\n",
            "Merged and saved file for protocol: grailers.eth\n",
            "Merged and saved file for protocol: grayboys.eth\n",
            "Merged and saved file for protocol: loot-dao.eth\n",
            "Merged and saved file for protocol: mutantsdao.eth\n",
            "Merged and saved file for protocol: nation3.eth\n",
            "Merged and saved file for protocol: thedreamdao.eth\n",
            "Merged and saved file for protocol: thelanddaoprop.eth\n",
            "Merging and saving completed.\n"
          ]
        }
      ],
      "source": [
        "# Merging files\n",
        "file1_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\merged_data_files\\preprocessing_voting_concat\\NFT\" \n",
        "file2_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\merged_data_files\\preprocessing111\\NFT\"\n",
        "\n",
        "# Specify the output directory for merged files\n",
        "output_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\merged_data_files\\NFT\"\n",
        "\n",
        "protocols = list(set(file.split('_')[0] for file in os.listdir(file1_directory) if file.endswith('.csv')))\n",
        "protocols.sort()\n",
        "\n",
        "for protocol in protocols:\n",
        "    # File 1\n",
        "    file1_name = f\"{protocol}_voting_data_concat.csv\"\n",
        "    file1_path = os.path.join(file1_directory, file1_name)\n",
        "\n",
        "    # File 2\n",
        "    file2_name = f\"{protocol}_preprocessing_intersected.csv\"\n",
        "    file2_path = os.path.join(file2_directory, file2_name)\n",
        "\n",
        "    # Output file\n",
        "    output_file_name = f\"{protocol}_merged_data.csv\"\n",
        "    output_file_path = os.path.join(output_directory, output_file_name)\n",
        "\n",
        "    # Read file1\n",
        "    df_file1 = pd.read_csv(file1_path,encoding='ISO-8859-1')\n",
        "    df_file1.rename(columns={'address': 'voter_address'}, inplace=True, errors='ignore')\n",
        "    df_file1['proposal_id'] = df_file1['proposal_id'].astype(str).str.lower().str.strip()\n",
        "\n",
        "    # Drop rows where 'choice' column is empty\n",
        "    # df_file1 = df_file1.dropna(subset=['choice'])\n",
        "\n",
        "    # Read file2\n",
        "    df_file2 = pd.read_csv(file2_path, encoding='ISO-8859-1')\n",
        "    \n",
        "\n",
        "    # Rename 'id' column to 'proposal_id' and drop 'network' and 'space' columns from file2\n",
        "    df_file2.rename(columns={'id': 'proposal_id'}, inplace=True, errors='ignore')\n",
        "    df_file2['proposal_id'] = df_file2['proposal_id'].astype(str).str.lower().str.strip()\n",
        "    # df_file2 = df_file2.drop(['network', 'space'], axis=1, errors='ignore')\n",
        "\n",
        "    # Merge file1 and file2 on 'proposal_id'\n",
        "    df_merged = pd.merge(df_file1, df_file2, on='proposal_id', how='left')\n",
        "    df_merged.dropna(subset=['title'], inplace=True)\n",
        "    df_merged['relative_time_voting'] = (df_merged['timestamp'] - df_merged['start']) / (df_merged['end'] - df_merged['start'])\n",
        "\n",
        "    # Save the merged DataFrame to a new CSV file\n",
        "    df_merged.to_csv(output_file_path, index=False)\n",
        "\n",
        "    print(f\"Merged and saved file for protocol: {protocol}\")\n",
        "\n",
        "print(\"Merging and saving completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged and saved file for protocol: aave\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "#####\n",
        "# filtering_intersection : merge with boardroom \n",
        "dir1 = \"C:/Users/ChunghyunHan/Desktop/DAO_Voting_1101/merged_data_files/ERC20_BRC20_FT\"\n",
        "# File directory 2\n",
        "dir2 = \"C:/Users/ChunghyunHan/Desktop/DAO_Voting_1101/Boardroom&Snapshot_data/Boardroom_proposal_data_contents\"\n",
        "# Output directory\n",
        "output_dir = \"C:/Users/ChunghyunHan/Desktop/DAO_Voting_1101/merged_data_files/ERC20_BRC20_FT/Merged_Boardroom_Snapshot\"\n",
        "\n",
        "def extract_columns(file_path):\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)\n",
        "    protocol_name = file_path.split('_')[0]\n",
        "    df.rename(columns={'id': 'proposal_id'}, inplace=True)\n",
        "    df['proposal_id'] = df['proposal_id'].astype(str).str.lower().str.strip()\n",
        "    return df[['proposal_id', 'blockNumber']], protocol_name\n",
        "\n",
        "\n",
        "# Get the list of files in both directories\n",
        "files_dir1 = [f for f in os.listdir(dir1) if f.endswith('.csv')]\n",
        "files_dir2 = [f for f in os.listdir(dir2) if f.endswith('.csv')]\n",
        "\n",
        "# Iterate through files in directory 1\n",
        "for file1 in files_dir1[2:3]:\n",
        "    # Extract protocol name from file name\n",
        "    protocol_name = file1.split('_')[0].split('.')[0]\n",
        "    \n",
        "    # Find matching file in directory 2\n",
        "    matching_file2 = next((f for f in files_dir2 if f.startswith(protocol_name)), None)\n",
        "\n",
        "    if matching_file2:\n",
        "        # Extract relevant columns from directory 2\n",
        "        df_dir2, protocol_name_dir2 = extract_columns(os.path.join(dir2, matching_file2))\n",
        "\n",
        "        # Read and extract relevant columns from directory 1\n",
        "        df_dir1 = pd.read_csv(os.path.join(dir1, file1), encoding='ISO-8859-1', low_memory=False)\n",
        "        df_dir2['proposal_id'] = df_dir2['proposal_id'].astype(str).str.lower().str.strip()\n",
        "        df_merged = pd.merge(df_dir1, df_dir2, on='proposal_id', how='inner')\n",
        "\n",
        "        # Save the merged DataFrame to a new CSV file in the output directory\n",
        "        output_file_path = os.path.join(output_dir, f\"{protocol_name}.eth_merged_data_intersected.csv\")\n",
        "        df_merged.to_csv(output_file_path, index=False)\n",
        "\n",
        "        print(f\"Merged and saved file for protocol: {protocol_name}\")\n",
        "    else:\n",
        "        print(f\"No matching file found for {file1} in directory 2.\")\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0xgov.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "1inch.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "aave.eth_merged_data.csv has been updated with new columns and saved back.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ChunghyunHan\\AppData\\Local\\Temp\\ipykernel_29476\\3409468978.py:13: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(os.path.join(directory, csv_file))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "aavegotchi.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "abachi.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "abracadabrabymerlinthemagician.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "alchemistcoin.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "alchemixstakers.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "alpacafinance.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "ampleforthorg.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "apecoin.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "apwine.eth_merged_data.csv has been updated with new columns and saved back.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ChunghyunHan\\AppData\\Local\\Temp\\ipykernel_29476\\3409468978.py:13: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(os.path.join(directory, csv_file))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "arbitrumfoundation.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "babydogevote.eth_merged_data.csv has been updated with new columns and saved back.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ChunghyunHan\\AppData\\Local\\Temp\\ipykernel_29476\\3409468978.py:13: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(os.path.join(directory, csv_file))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "badgerdao.eth_merged_data.csv has been updated with new columns and saved back.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ChunghyunHan\\AppData\\Local\\Temp\\ipykernel_29476\\3409468978.py:13: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(os.path.join(directory, csv_file))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "balancer.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "bancornetwork.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "banklessvault.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "barnbridge.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "beanstalkdao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "beanstalkfarms.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "beets.eth_merged_data.csv has been updated with new columns and saved back.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ChunghyunHan\\AppData\\Local\\Temp\\ipykernel_29476\\3409468978.py:13: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(os.path.join(directory, csv_file))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bestfork.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "bitdao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "bitembassy.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "blockzerolabs.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "botto.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "cabindao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "comp-vote.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "conic-dao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "cow.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "cre8r.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "cream-finance.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "curve.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "daomstr.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "decentraland.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "decentralgames.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "devdao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "dhedge.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "dorg.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "dunjia.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "dydxgov.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "elementdao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "elyfi.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "ens.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "esd.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "eulerdao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "fei.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "forefront.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "frax.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "friendswithbenefits.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "gasdao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "gearbox.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "gitcoindao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "gnosis.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "groxyz.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "hashflowdao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "hectordao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "hop.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "idlefinance.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "index-coop.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "indexed.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "jadeprotocol.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "joegovernance.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "jpegd.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "juicebox.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "kleros.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "klimadao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "krausehouse.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "lgcryptounicorns.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "lido-snapshot.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "mantra-dao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "meritcircle.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "metacartel.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "metafactory.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "metislayer2.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "morpho.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "mstablegovernance.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "nexusmutual.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "notional.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "officialoceandao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "olympusdao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "opiumprotocol.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "optimism.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "paraswap-dao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "people-dao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "perpetualprotocol.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "phonon.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "pickle.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "piedao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "poh.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "poolpoolpooltogether.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "pooltogether.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "popcorn-snapshot.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "premia.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "pushdao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "qidao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "radiantcapital.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "rallygov.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "rari.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "rarible.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "redactedcartel.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "ribbonfi.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "rocketpool-dao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "saddlefinance.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "shapeshiftdao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "sharkdao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "shellprotocol.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "silofinance.eth_merged_data.csv has been updated with new columns and saved back.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ChunghyunHan\\AppData\\Local\\Temp\\ipykernel_29476\\3409468978.py:13: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(os.path.join(directory, csv_file))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speraxdao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "spiralgov.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "spookyswap.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "stakedao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "streamr.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "sushigov.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "tecommons.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "theopendao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "threshold.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "tokenlon.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "tomoondao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "tracer.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "truefigov.eth_merged_data.csv has been updated with new columns and saved back.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ChunghyunHan\\AppData\\Local\\Temp\\ipykernel_29476\\3409468978.py:13: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(os.path.join(directory, csv_file))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "uniswap.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "unlock-protocol.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "usebraintrust.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "varen.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "vote-airswap.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "vote-vitadao.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "vsp.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "xdaistake.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "yam.eth_merged_data.csv has been updated with new columns and saved back.\n",
            "ybaby.eth_merged_data.csv has been updated with new columns and saved back.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'DONE'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## accountability column building in merged files\n",
        "\n",
        "import pandas as pd\n",
        "import os \n",
        "\n",
        "def process_csv_files(directory):\n",
        "    # List all csv files in the directory\n",
        "    csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
        "\n",
        "    # Process each csv file\n",
        "    for csv_file in csv_files:\n",
        "        # Read the csv file\n",
        "        df = pd.read_csv(os.path.join(directory, csv_file))\n",
        "        \n",
        "        # Calculate the accountability for each voter in each proposal\n",
        "        df['accountability'] = df['voting_power'] / df.groupby('proposal_id')['voting_power'].transform('sum')\n",
        "        \n",
        "        # Rank each voter in each proposal by their accountability\n",
        "        df['voter_rank'] = df.groupby('proposal_id')['accountability'].rank(ascending=False, method='min')\n",
        "        \n",
        "        # Count the total number of voters in each proposal\n",
        "        df['total_voters'] = df.groupby('proposal_id')['voter_address'].transform('nunique')\n",
        "        \n",
        "        # Save the modified dataframe back to csv\n",
        "        df.to_csv(os.path.join(directory, csv_file), index=False)\n",
        "\n",
        "        print(f\"{csv_file} has been updated with new columns and saved back.\")\n",
        "\n",
        "    return 'DONE'\n",
        "\n",
        "# Replace with the path to your CSV files\n",
        "directory = r'D:\\DAO_Voting_1101\\merged_data_files\\FT'\n",
        "process_csv_files(directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### basic stats\n",
        "1) unique_voters, authors\n",
        "2) winrate\n",
        "3) author stats : top 1~3 authors\n",
        "4) VP_gini coefficient\n",
        "5) num voters to achieve 50%\n",
        "6) voter stats : unique voters count\n",
        "7) participation rate (out of all voter set)\n",
        "8) top actively participating voters\n",
        "9) turnover ratio (two consecutive proposals)\n",
        "10) whale top 5 address, voter accountability, voting timing (other voters' voting timing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. basic stats\n",
        "# Input directory containing CSV files\n",
        "input_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\merged_data_files\\FT\"\n",
        "\n",
        "# Create an empty list to store the statistics\n",
        "stats_list = []\n",
        "# cream, decentraland, rallygov\n",
        "\n",
        "# Iterate through each CSV file in the input directory\n",
        "for file_name in os.listdir(input_directory):\n",
        "    if file_name.endswith('.csv'):\n",
        "        # Extract protocol_name from the file name\n",
        "        protocol_name = file_name.split('_')[0].split('.')[0]\n",
        "\n",
        "        # Read the CSV file into a DataFrame\n",
        "        file_path = os.path.join(input_directory, file_name)\n",
        "        df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)\n",
        "        df = df.dropna(subset=['choice'])\n",
        "\n",
        "        # Count unique values in the 'proposal_id' column\n",
        "        proposals_count = df['proposal_id'].nunique()\n",
        "        unique_voters_count = df['voter_address'].nunique()\n",
        "        unique_authors_count = df['author'].nunique()\n",
        "       \n",
        "        # win_rate stats : DF\n",
        "        grouped_winrate = df.groupby('proposal_id')['Winrate'].agg(['min']) # all the same values for each group-> extract using 'min'\n",
        "        grouped_winrate.rename(columns={'min': 'winrate_by_proposal'}, inplace=True)\n",
        "\n",
        "        # NUM_votes stats : Series\n",
        "        num_votes_valid = df.groupby('proposal_id').size()\n",
        "        num_votes_valid = num_votes_valid.rename('num_votes_valid').reset_index()\n",
        "\n",
        "\n",
        "        # Add a new dictionary to the list\n",
        "        stats_list.append({\n",
        "            'protocol_name': protocol_name,\n",
        "            'proposals_count': proposals_count,\n",
        "            'unique_voters_count': unique_voters_count,  \n",
        "            'unique_authors_count': unique_authors_count,\n",
        "            'min_win_rate': grouped_winrate['winrate_by_proposal'].min(),\n",
        "            '25th_per_win_rate': grouped_winrate['winrate_by_proposal'].quantile(0.25),\n",
        "            'median_win_rate': grouped_winrate['winrate_by_proposal'].median(),\n",
        "            '75th_per_win_rate': grouped_winrate['winrate_by_proposal'].quantile(0.75),\n",
        "            'max_win_rate': grouped_winrate['winrate_by_proposal'].max(),\n",
        "            'avg_win_rate': grouped_winrate['winrate_by_proposal'].mean(),\n",
        "            'std_win_rate' : grouped_winrate['winrate_by_proposal'].std(),\n",
        "            'Sum_valid_votes' : num_votes_valid['num_votes_valid'].sum(),\n",
        "            'min_num_votes': num_votes_valid['num_votes_valid'].min(),\n",
        "            '25th_percentile_num_votes': num_votes_valid['num_votes_valid'].quantile(0.25),\n",
        "            'median_num_votes': num_votes_valid['num_votes_valid'].median(),\n",
        "            '75th_percentile_num_vote': num_votes_valid['num_votes_valid'].quantile(0.75),\n",
        "            'max_num_votes': num_votes_valid['num_votes_valid'].max(),\n",
        "            'avg_num_votes' : num_votes_valid['num_votes_valid'].mean(),\n",
        "            'std_num_votes' : num_votes_valid['num_votes_valid'].std()\n",
        "        })\n",
        "\n",
        "# Create the statistics DataFrame\n",
        "stats_df = pd.DataFrame(stats_list)\n",
        "\n",
        "## Save the statistics DataFrame to a CSV file\n",
        "#output_file_path = os.path.join(output_directory, 'proposals_statistics.csv')\n",
        "#stats_df.to_csv(output_file_path, index=False)\n",
        "#print(f\"Statistics saved to: {output_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output directory for the statistics CSV file\n",
        "output_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\Statistics\"\n",
        "output_file_path = os.path.join(output_directory, 'FT_statistics.xlsx')\n",
        "with pd.ExcelWriter(output_file_path, engine='xlsxwriter') as writer:\n",
        "    # Write the first sheet\n",
        "    stats_df[['protocol_name', 'proposals_count', 'unique_voters_count', 'unique_authors_count', 'min_win_rate', '25th_per_win_rate', 'median_win_rate',\n",
        "              '75th_per_win_rate', 'max_win_rate', 'avg_win_rate', 'std_win_rate']].to_excel(writer, sheet_name='proposal_voter_authors_winrate', index=False)\n",
        "\n",
        "    # Write the second sheet\n",
        "    stats_df[['protocol_name', 'Sum_valid_votes', 'min_num_votes', '25th_percentile_num_votes',\n",
        "              'median_num_votes', '75th_percentile_num_vote', 'max_num_votes', 'avg_num_votes', 'std_num_votes']].to_excel(writer, sheet_name='votes_stats_by_proposal', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. author stats\n",
        "# Directory containing CSV files\n",
        "directory_path = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\merged_data_files\\NFT\"\n",
        "\n",
        "# Initialize an empty list to store the statistics\n",
        "author_stats_list = []\n",
        "\n",
        "# Initialize lists to store top authors for each protocol\n",
        "top_author_1_list, top_author_2_list, top_author_3_list = [], [], []\n",
        "\n",
        "# Iterate through each file in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        try:\n",
        "            # Extract protocol name from the file name\n",
        "            protocol_name = filename.split('_')[0]\n",
        "\n",
        "            # Read the CSV file into a DataFrame\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)\n",
        "\n",
        "            # Convert columns to lowercase and strip whitespace\n",
        "            df['proposal_id'] = df['proposal_id'].astype(str).str.lower().str.strip()\n",
        "            df['author'] = df['author'].astype(str).str.lower().str.strip()\n",
        "\n",
        "            all_authors = []  # Initialize list to store all authors for each protocol\n",
        "\n",
        "            # Group by 'proposal_id' and create a DataFrame for each group\n",
        "            for proposal_id, group_df in df.groupby('proposal_id'):\n",
        "                # Create a new DataFrame with one row for the group\n",
        "                group_data = {'proposal_id': proposal_id, 'author': group_df['author'].iloc[0]}\n",
        "                group_stats_df = pd.DataFrame([group_data])\n",
        "\n",
        "                # Print or append the DataFrame to the list as needed\n",
        "                authors_list = group_stats_df['author'].tolist()\n",
        "                all_authors.extend(authors_list)\n",
        "\n",
        "            author_counts = dict(Counter(all_authors))\n",
        "            top_authors = [author for author, _ in Counter(all_authors).most_common(3)]\n",
        "\n",
        "            # Append to the corresponding lists\n",
        "            top_author_1_list.append(top_authors[0] if len(top_authors) > 0 else None)\n",
        "            top_author_2_list.append(top_authors[1] if len(top_authors) > 1 else None)\n",
        "            top_author_3_list.append(top_authors[2] if len(top_authors) > 2 else None)\n",
        "\n",
        "            # Create a dictionary of author values, their occurrence count, and top authors\n",
        "            author_stats = {\n",
        "                'protocol_name': protocol_name,\n",
        "                'unique_authors': len(author_counts),\n",
        "                'occur_dict': author_counts,\n",
        "                'top_author_1': top_author_1_list[-1],  # Use the last value in the list\n",
        "                'top_1_occurrence': author_counts.get(top_author_1_list[-1], {}),\n",
        "                'top_author_2': top_author_2_list[-1],\n",
        "                'top_2_occurrence': author_counts.get(top_author_2_list[-1], {}),\n",
        "                'top_author_3': top_author_3_list[-1],\n",
        "                'top_3_occurrence': author_counts.get(top_author_3_list[-1], {})\n",
        "            }\n",
        "\n",
        "            # Append the statistics to the list\n",
        "            author_stats_list.append(author_stats)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file: {file_path}\\nError: {str(e)}\")\n",
        "\n",
        "author_stats_df = pd.DataFrame(author_stats_list)\n",
        "author_stats_df.to_excel(r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\Statistics\\NFT_author_stats.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Directory containing CSV files\n",
        "directory_path = r'C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\merged_data_files\\FT'\n",
        "\n",
        "# Initialize an empty dictionary to store the voter and author sets for each protocol\n",
        "voter_sets_by_protocol = {}\n",
        "author_sets_by_protocol = {}\n",
        "\n",
        "# Iterate through each file in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        # Extract protocol name from the file name\n",
        "        protocol_name = filename.split('_')[0].split('.')[0]\n",
        "\n",
        "        # Read the CSV file into a DataFrame\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)\n",
        "\n",
        "        # Get the full voter set for the current protocol (case-insensitive)\n",
        "        voter_set = set(df['voter_address'].str.lower())\n",
        "        voter_sets_by_protocol[protocol_name] = voter_set\n",
        "\n",
        "        # Get the full author set for the current protocol (case-insensitive)\n",
        "        author_set = set(df['author'].str.lower())\n",
        "        author_sets_by_protocol[protocol_name] = author_set\n",
        "\n",
        "# Initialize empty dictionaries to store the intersection between any two protocols\n",
        "intersection_dict = {}\n",
        "\n",
        "# Compare the sets for each pair of protocols\n",
        "protocols = list(voter_sets_by_protocol.keys())\n",
        "for i in range(len(protocols)):\n",
        "    for j in range(i + 1, len(protocols)):\n",
        "        protocol1 = protocols[i]\n",
        "        protocol2 = protocols[j]\n",
        "\n",
        "        # Find the intersection between the two sets for 'voter_address' (case-insensitive)\n",
        "        voter_intersection = voter_sets_by_protocol[protocol1].intersection(voter_sets_by_protocol[protocol2])\n",
        "\n",
        "        # Find the intersection between the two sets for 'author' (case-insensitive)\n",
        "        author_intersection = author_sets_by_protocol[protocol1].intersection(author_sets_by_protocol[protocol2])\n",
        "\n",
        "        # Store the intersections and their lengths in the result dictionary\n",
        "        intersection_dict[f'{protocol1} - {protocol2}'] = {\n",
        "            'Protocol 1 Voter Set Length': len(voter_sets_by_protocol[protocol1]),\n",
        "            'Protocol 2 Voter Set Length': len(voter_sets_by_protocol[protocol2]),\n",
        "            'Voter Intersection': list(voter_intersection),\n",
        "            'Voter Intersection Length': len(voter_intersection),\n",
        "            'Author Intersection': list(author_intersection),\n",
        "            'Author Intersection Length': len(author_intersection)\n",
        "        }\n",
        "\n",
        "# Create a DataFrame from the result dictionary\n",
        "result_df = pd.DataFrame(intersection_dict).T.reset_index()\n",
        "result_df.rename(columns={'index': 'Protocol Pair'}, inplace=True)\n",
        "\n",
        "# Save to a single Excel file\n",
        "result_df.to_excel(r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\Statistics\\voter_author_intersection_FT.xlsx\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os \n",
        "import pandas as pd\n",
        "# voter stats\n",
        "\n",
        "# Specify the directory containing the files\n",
        "directory_path = r'C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\merged_data_files\\FT'\n",
        "\n",
        "# Initialize an empty list to store the statistics\n",
        "stats_list = []\n",
        "\n",
        "# Iterate through each file in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        protocol_name = filename.split(\"_\")[0].split(\".\")[0]\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)\n",
        "\n",
        "        full_voter_set = set(df['voter_address'])\n",
        "\n",
        "        proposal_voters_dict = {}\n",
        "        for _, row in df.iterrows():\n",
        "            proposal_id = row['proposal_id']\n",
        "            voter_address = row['voter_address']\n",
        "\n",
        "            if proposal_id not in proposal_voters_dict:\n",
        "                proposal_voters_dict[proposal_id] = set()\n",
        "\n",
        "            proposal_voters_dict[proposal_id].add(voter_address)\n",
        "\n",
        "        participation_rate_dict = {}\n",
        "        for proposal_id, voters_in_proposal in proposal_voters_dict.items():\n",
        "            participation_rate = len(voters_in_proposal) / len(full_voter_set)\n",
        "            participation_rate_dict[proposal_id] = participation_rate\n",
        "\n",
        "        voter_participation_count = {}\n",
        "        for voter in full_voter_set:\n",
        "            count = sum(1 for voters_in_proposal in proposal_voters_dict.values() if voter in voters_in_proposal)\n",
        "            voter_participation_count[voter] = count\n",
        "\n",
        "        highest_parti_rate_proposal = max(participation_rate_dict, key=participation_rate_dict.get)\n",
        "        highest_participation_rate = participation_rate_dict[highest_parti_rate_proposal]\n",
        "        avg_participation_rate = sum(participation_rate_dict.values()) / len(participation_rate_dict)\n",
        "        min_participation_rate = min(participation_rate_dict.values())\n",
        "\n",
        "        top_participating_voters = sorted(voter_participation_count.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "        unique_proposal_ids = len(set(df['proposal_id']))\n",
        "\n",
        "\n",
        "        sort_parti_rate = sorted(participation_rate_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "        top_50_parti_rate = dict(sort_parti_rate[:50])\n",
        "\n",
        "        sort_voter_participation_count = sorted(voter_participation_count.items(), key=lambda x: x[1], reverse=True)\n",
        "        top_50_voter_participation_count = dict(sort_voter_participation_count[:50])\n",
        "\n",
        "        # Add additional columns to the result dictionary\n",
        "        result = {\n",
        "            'protocol_name': protocol_name,\n",
        "            'full_voter_set_size': len(full_voter_set),\n",
        "            'unique_proposal_ids': unique_proposal_ids,\n",
        "            'top_50_participation_rate_dict': top_50_parti_rate,\n",
        "            'top50_voter_participation_count':top_50_voter_participation_count,\n",
        "            'highest_participation_proposal': highest_parti_rate_proposal,\n",
        "            'avg_participation_rate': avg_participation_rate,\n",
        "            'min_participation_rate': min_participation_rate,\n",
        "            'highest_participation_rate': highest_participation_rate,\n",
        "            'top5_participating_voters': [voter for voter, _ in top_participating_voters],\n",
        "            'top5_participating_counts': [count for _, count in top_participating_voters]\n",
        "        }\n",
        "\n",
        "        stats_list.append(result)\n",
        "\n",
        "# Create a DataFrame from the list of statistics\n",
        "stats_df = pd.DataFrame(stats_list)\n",
        "\n",
        "# Print the DataFrame\n",
        "stats_df.to_excel(r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\Statistics\\voter_stats_FT.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stats for each proposal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved voter stats for 0xgov\n",
            "Saved voter stats for 1inch\n",
            "Saved voter stats for aave\n",
            "Saved voter stats for aavegotchi\n",
            "Saved voter stats for abachi\n",
            "Saved voter stats for abracadabrabymerlinthemagician\n",
            "Saved voter stats for alchemistcoin\n",
            "Saved voter stats for alchemixstakers\n",
            "Saved voter stats for alpacafinance\n",
            "Saved voter stats for ampleforthorg\n",
            "Saved voter stats for apecoin\n",
            "Saved voter stats for apwine\n",
            "Saved voter stats for arbitrumfoundation\n",
            "Saved voter stats for babydogevote\n",
            "Saved voter stats for badgerdao\n",
            "Saved voter stats for balancer\n",
            "Saved voter stats for bancornetwork\n",
            "Saved voter stats for banklessvault\n",
            "Saved voter stats for barnbridge\n",
            "Saved voter stats for beanstalkdao\n",
            "Saved voter stats for beanstalkfarms\n",
            "Saved voter stats for beets\n",
            "Saved voter stats for bestfork\n",
            "Saved voter stats for bitdao\n",
            "Saved voter stats for bitembassy\n",
            "Saved voter stats for blockzerolabs\n",
            "Saved voter stats for botto\n",
            "Saved voter stats for cabindao\n",
            "Saved voter stats for comp-vote\n",
            "Saved voter stats for conic-dao\n",
            "Saved voter stats for cow\n",
            "Saved voter stats for cre8r\n",
            "Saved voter stats for cream-finance\n",
            "Saved voter stats for curve\n",
            "Saved voter stats for daomstr\n",
            "Saved voter stats for decentraland\n",
            "Saved voter stats for decentralgames\n",
            "Saved voter stats for devdao\n",
            "Saved voter stats for dhedge\n",
            "Saved voter stats for dorg\n",
            "Saved voter stats for dunjia\n",
            "Saved voter stats for dydxgov\n",
            "Saved voter stats for elementdao\n",
            "Saved voter stats for elyfi\n",
            "Saved voter stats for ens\n",
            "Saved voter stats for esd\n",
            "Saved voter stats for eulerdao\n",
            "Saved voter stats for fei\n",
            "Saved voter stats for forefront\n",
            "Saved voter stats for frax\n",
            "Saved voter stats for friendswithbenefits\n",
            "Saved voter stats for gasdao\n",
            "Saved voter stats for gearbox\n",
            "Saved voter stats for gitcoindao\n",
            "Saved voter stats for gnosis\n",
            "Saved voter stats for groxyz\n",
            "Saved voter stats for hashflowdao\n",
            "Saved voter stats for hectordao\n",
            "Saved voter stats for hop\n",
            "Saved voter stats for idlefinance\n",
            "Saved voter stats for index-coop\n",
            "Saved voter stats for indexed\n",
            "Saved voter stats for jadeprotocol\n",
            "Saved voter stats for joegovernance\n",
            "Saved voter stats for jpegd\n",
            "Saved voter stats for juicebox\n",
            "Saved voter stats for kleros\n",
            "Saved voter stats for klimadao\n",
            "Saved voter stats for krausehouse\n",
            "Saved voter stats for lgcryptounicorns\n",
            "Saved voter stats for lido-snapshot\n",
            "Saved voter stats for mantra-dao\n",
            "Saved voter stats for meritcircle\n",
            "Saved voter stats for metacartel\n",
            "Saved voter stats for metafactory\n",
            "Saved voter stats for metislayer2\n",
            "Saved voter stats for morpho\n",
            "Saved voter stats for mstablegovernance\n",
            "Saved voter stats for nexusmutual\n",
            "Saved voter stats for notional\n",
            "Saved voter stats for officialoceandao\n",
            "Saved voter stats for olympusdao\n",
            "Saved voter stats for opiumprotocol\n",
            "Saved voter stats for optimism\n",
            "Saved voter stats for paraswap-dao\n",
            "Saved voter stats for people-dao\n",
            "Saved voter stats for perpetualprotocol\n",
            "Saved voter stats for phonon\n",
            "Saved voter stats for pickle\n",
            "Saved voter stats for piedao\n",
            "Saved voter stats for poh\n",
            "Saved voter stats for poolpoolpooltogether\n",
            "Saved voter stats for pooltogether\n",
            "Saved voter stats for popcorn-snapshot\n",
            "Saved voter stats for premia\n",
            "Saved voter stats for pushdao\n",
            "Saved voter stats for qidao\n",
            "Saved voter stats for radiantcapital\n",
            "Saved voter stats for rallygov\n",
            "Saved voter stats for rari\n",
            "Saved voter stats for rarible\n",
            "Saved voter stats for redactedcartel\n",
            "Saved voter stats for ribbonfi\n",
            "Saved voter stats for rocketpool-dao\n",
            "Saved voter stats for saddlefinance\n",
            "Saved voter stats for shapeshiftdao\n",
            "Saved voter stats for sharkdao\n",
            "Saved voter stats for shellprotocol\n",
            "Saved voter stats for silofinance\n",
            "Saved voter stats for speraxdao\n",
            "Saved voter stats for spiralgov\n",
            "Saved voter stats for spookyswap\n",
            "Saved voter stats for stakedao\n",
            "Saved voter stats for streamr\n",
            "Saved voter stats for sushigov\n",
            "Saved voter stats for tecommons\n",
            "Saved voter stats for theopendao\n",
            "Saved voter stats for threshold\n",
            "Saved voter stats for tokenlon\n",
            "Saved voter stats for tomoondao\n",
            "Saved voter stats for tracer\n",
            "Saved voter stats for truefigov\n",
            "Saved voter stats for uniswap\n",
            "Saved voter stats for unlock-protocol\n",
            "Saved voter stats for usebraintrust\n",
            "Saved voter stats for varen\n",
            "Saved voter stats for vote-airswap\n",
            "Saved voter stats for vote-vitadao\n",
            "Saved voter stats for vsp\n",
            "Saved voter stats for xdaistake\n",
            "Saved voter stats for yam\n",
            "Saved voter stats for ybaby\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate Gini coefficient\n",
        "def calculate_gini_coefficient(voting_power_list):\n",
        "    sorted_voting_power = np.sort(voting_power_list)\n",
        "    cumulative_sum = np.cumsum(sorted_voting_power)\n",
        "    cumulative_proportion = cumulative_sum / np.sum(sorted_voting_power)\n",
        "    n = len(sorted_voting_power)\n",
        "    gini_coefficient = 1 - (2 * np.sum((n - np.arange(1, n + 1)) * cumulative_proportion) / (n * np.sum(sorted_voting_power)))\n",
        "    return gini_coefficient\n",
        "\n",
        "# Function to calculate the number of voters needed to achieve over 50%\n",
        "def calculate_voters_needed_50_percent(sorted_voters):\n",
        "    total_voting_power = sorted_voters['voting_power'].sum()\n",
        "    cumulative_voting_power = 0\n",
        "    voters_needed = 0\n",
        "\n",
        "    for _, row in sorted_voters.iterrows():\n",
        "        cumulative_voting_power += row['voting_power']\n",
        "        voters_needed += 1\n",
        "\n",
        "        if total_voting_power == 0 or (cumulative_voting_power / total_voting_power > 0.5):\n",
        "            break\n",
        "\n",
        "    return voters_needed\n",
        "\n",
        "# Function to calculate HHI\n",
        "def calculate_hhi(voting_power_list):\n",
        "    total_voting_power = np.sum(voting_power_list)\n",
        "    share_squared = np.sum((voting_power_list / total_voting_power) ** 2)\n",
        "    return share_squared\n",
        "\n",
        "# Function to calculate normalized HHI\n",
        "def calculate_normalized_hhi(voting_power_list):\n",
        "    n = len(voting_power_list)\n",
        "    if n <= 1:  # Avoid division by zero or negative normalization for n=1\n",
        "        return 0\n",
        "    hhi = calculate_hhi(voting_power_list)\n",
        "    normalized_hhi = (hhi - (1/n)) / (1 - (1/n))\n",
        "    return normalized_hhi\n",
        "\n",
        "\n",
        "def calculate_cr(sorted_voting_powers, top_n):\n",
        "    if len(sorted_voting_powers) < top_n:\n",
        "        # If there are fewer members than top_n, calculate CR for all available members\n",
        "        top_n = len(sorted_voting_powers)\n",
        "    return sum(sorted_voting_powers[:top_n]) / sum(sorted_voting_powers)\n",
        "\n",
        "# Specify the directory containing the files\n",
        "directory_path = r'D:\\DAO_Voting_1101\\merged_data_files\\FT'\n",
        "output_directory = r'D:\\DAO_Voting_1101\\Statistics\\voter_stats\\participation_rate&gini_coef&voters_needed_achieve50%_byproposal'\n",
        "all_voters_cumulative = set()  # For cumulative participation rate across all proposals\n",
        "\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        protocol_name = filename.split(\"_\")[0].split(\".\")[0]\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)\n",
        "        df['voting_power'] = pd.to_numeric(df['voting_power'], errors='coerce')\n",
        "        df = df.dropna(subset=['voting_power'])  # Drop NaN values first to avoid comparison with NaN\n",
        "        df = df[df['voting_power'] > 0]  # Keep rows where 'voting_power' is greater than 0\n",
        "\n",
        "        # Ensure proposals are processed in chronological order\n",
        "        df = df.sort_values(by='start')\n",
        "        \n",
        "        results = []\n",
        "\n",
        "        for proposal_id, group_df in df.groupby('proposal_id'):\n",
        "            group_df = group_df.sort_values(by='voting_power', ascending=False)  # Sort by voting power for CR calculation\n",
        "            current_voters = set(group_df['voter_address'])\n",
        "            all_voters_cumulative.update(current_voters)\n",
        "            \n",
        "            # Metrics calculation\n",
        "            gini_coefficient = calculate_gini_coefficient(group_df['voting_power'].values)\n",
        "            voters_needed = calculate_voters_needed_50_percent(group_df)\n",
        "            hhi = calculate_hhi(group_df['voting_power'].values)\n",
        "            normalized_hhi = calculate_normalized_hhi(group_df['voting_power'].values)\n",
        "            \n",
        "            cr_top_1 = calculate_cr(group_df['voting_power'], 1)\n",
        "            cr_top_3 = calculate_cr(group_df['voting_power'], 3)\n",
        "            cr_top_5 = calculate_cr(group_df['voting_power'], 5)\n",
        "            cr_top_7 = calculate_cr(group_df['voting_power'], 7)\n",
        "            cr_top_10 = calculate_cr(group_df['voting_power'], 10)\n",
        "            \n",
        "            total_voters = len(current_voters)  # Size of the total voter set for each group\n",
        "            simple_participation_rate = len(current_voters) / len(set(df['voter_address']))\n",
        "            cumulative_participation_rate = len(all_voters_cumulative) / len(set(df['voter_address']))\n",
        "            start_time = group_df['start'].iloc[0]\n",
        "\n",
        "            results.append({\n",
        "                'proposal_id': proposal_id,\n",
        "                'simple_participation_rate': simple_participation_rate,\n",
        "                'cumulative_participation_rate': cumulative_participation_rate,\n",
        "                'gini_coefficient': gini_coefficient,\n",
        "                'voters_needed_50_percent': voters_needed,\n",
        "                'HHI': hhi,\n",
        "                'normalized_HHI': normalized_hhi,\n",
        "                'CR_top_1': cr_top_1,\n",
        "                'CR_top_3': cr_top_3,\n",
        "                'CR_top_5': cr_top_5,\n",
        "                'CR_top_7': cr_top_7,\n",
        "                'CR_top_10': cr_top_10,\n",
        "                'total_voters': total_voters,\n",
        "                'start_time': start_time,\n",
        "                'start_real_time': pd.to_datetime(start_time, unit='s')\n",
        "            })\n",
        "\n",
        "        result_df = pd.DataFrame(results)\n",
        "        output_filename = f'{protocol_name}_voter_stats.csv'\n",
        "        output_path = os.path.join(output_directory, output_filename)\n",
        "        result_df.to_csv(output_path, index=False)\n",
        "        print(f\"Saved voter stats for {protocol_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# turnover rate\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the directory containing the files\n",
        "directory_path = r\"D:\\DAO_Voting_1101\\merged_data_files\\FT\"\n",
        "\n",
        "# Output directory\n",
        "output_directory = r\"D:\\DAO_Voting_1101\\Statistics\\voter_stats\\consecutive_two_turnover_ratio\"\n",
        "\n",
        "# Iterate through each file in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        # Read the CSV file\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)\n",
        "\n",
        "        # Sort DataFrame by 'start' column\n",
        "        df.sort_values(by='end', inplace=True)\n",
        "\n",
        "        # Turnover Rate Calculation\n",
        "        proposals = df['proposal_id'].unique()\n",
        "\n",
        "        turnover_ratios = {}\n",
        "        for i in range(1, len(proposals)):\n",
        "            current_proposal = proposals[i]\n",
        "            previous_proposal = proposals[i - 1]\n",
        "\n",
        "            current_voters = set(df[df['proposal_id'] == current_proposal]['voter_address'])\n",
        "            previous_voters = set(df[df['proposal_id'] == previous_proposal]['voter_address'])\n",
        "\n",
        "            intersection_count = len(current_voters.intersection(previous_voters))\n",
        "            union_count = len(current_voters.union(previous_voters))\n",
        "\n",
        "            turnover_ratio = intersection_count / union_count\n",
        "            turnover_ratios[f'{previous_proposal} -> {current_proposal}'] = turnover_ratio\n",
        "            \n",
        "        # Create DataFrame from turnover_ratios\n",
        "        result_df = pd.DataFrame(list(turnover_ratios.items()), columns=['proposal_pair','turnover_ratio'])\n",
        "\n",
        "        # Extract protocol_name from the filename\n",
        "        protocol_name = filename.split(\"_\")[0].split(\".\")[0]\n",
        "\n",
        "        # Save the DataFrame to CSV\n",
        "        output_filename = f'{protocol_name}_turnover_ratio.csv'\n",
        "        output_path = os.path.join(output_directory, output_filename)\n",
        "        result_df.to_csv(output_path, index=False)\n",
        "\n",
        "        print(f\"Processed file: {filename}, Turnover Ratios saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file: 0xgov.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\0xgov_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: 1inch.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\1inch_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: aave.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\aave_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: aavegotchi.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\aavegotchi_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: abachi.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\abachi_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: abracadabrabymerlinthemagician.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\abracadabrabymerlinthemagician_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: alchemistcoin.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\alchemistcoin_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: alchemixstakers.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\alchemixstakers_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: alpacafinance.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\alpacafinance_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: ampleforthorg.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\ampleforthorg_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: apecoin.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\apecoin_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: apwine.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\apwine_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: arbitrumfoundation.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\arbitrumfoundation_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: babydogevote.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\babydogevote_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: badgerdao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\badgerdao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: balancer.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\balancer_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: bancornetwork.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\bancornetwork_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: banklessvault.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\banklessvault_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: barnbridge.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\barnbridge_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: beanstalkdao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\beanstalkdao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: beanstalkfarms.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\beanstalkfarms_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: beets.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\beets_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: bestfork.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\bestfork_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: bitdao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\bitdao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: bitembassy.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\bitembassy_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: blockzerolabs.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\blockzerolabs_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: botto.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\botto_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: cabindao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\cabindao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: comp-vote.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\comp-vote_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: conic-dao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\conic-dao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: cow.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\cow_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: cre8r.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\cre8r_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: cream-finance.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\cream-finance_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: curve.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\curve_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: daomstr.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\daomstr_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: decentraland.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\decentraland_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: decentralgames.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\decentralgames_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: devdao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\devdao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: dhedge.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\dhedge_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: dorg.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\dorg_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: dunjia.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\dunjia_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: dydxgov.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\dydxgov_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: elementdao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\elementdao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: elyfi.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\elyfi_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: ens.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\ens_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: esd.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\esd_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: eulerdao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\eulerdao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: fei.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\fei_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: forefront.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\forefront_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: frax.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\frax_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: friendswithbenefits.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\friendswithbenefits_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: gasdao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\gasdao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: gearbox.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\gearbox_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: gitcoindao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\gitcoindao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: gnosis.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\gnosis_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: groxyz.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\groxyz_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: hashflowdao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\hashflowdao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: hectordao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\hectordao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: hop.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\hop_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: idlefinance.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\idlefinance_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: index-coop.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\index-coop_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: indexed.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\indexed_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: jadeprotocol.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\jadeprotocol_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: joegovernance.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\joegovernance_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: jpegd.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\jpegd_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: juicebox.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\juicebox_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: kleros.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\kleros_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: klimadao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\klimadao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: krausehouse.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\krausehouse_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: lgcryptounicorns.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\lgcryptounicorns_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: lido-snapshot.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\lido-snapshot_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: mantra-dao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\mantra-dao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: meritcircle.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\meritcircle_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: metacartel.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\metacartel_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: metafactory.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\metafactory_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: metislayer2.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\metislayer2_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: morpho.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\morpho_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: mstablegovernance.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\mstablegovernance_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: nexusmutual.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\nexusmutual_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: notional.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\notional_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: officialoceandao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\officialoceandao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: olympusdao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\olympusdao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: opiumprotocol.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\opiumprotocol_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: optimism.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\optimism_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: paraswap-dao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\paraswap-dao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: people-dao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\people-dao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: perpetualprotocol.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\perpetualprotocol_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: phonon.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\phonon_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: pickle.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\pickle_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: piedao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\piedao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: poh.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\poh_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: poolpoolpooltogether.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\poolpoolpooltogether_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: pooltogether.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\pooltogether_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: popcorn-snapshot.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\popcorn-snapshot_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: premia.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\premia_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: pushdao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\pushdao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: qidao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\qidao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: radiantcapital.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\radiantcapital_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: rallygov.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\rallygov_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: rari.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\rari_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: rarible.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\rarible_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: redactedcartel.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\redactedcartel_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: ribbonfi.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\ribbonfi_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: rocketpool-dao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\rocketpool-dao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: saddlefinance.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\saddlefinance_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: shapeshiftdao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\shapeshiftdao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: sharkdao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\sharkdao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: shellprotocol.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\shellprotocol_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: silofinance.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\silofinance_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: speraxdao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\speraxdao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: spiralgov.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\spiralgov_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: spookyswap.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\spookyswap_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: stakedao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\stakedao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: streamr.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\streamr_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: sushigov.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\sushigov_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: tecommons.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\tecommons_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: theopendao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\theopendao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: threshold.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\threshold_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: tokenlon.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\tokenlon_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: tomoondao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\tomoondao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: tracer.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\tracer_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: truefigov.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\truefigov_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: uniswap.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\uniswap_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: unlock-protocol.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\unlock-protocol_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: usebraintrust.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\usebraintrust_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: varen.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\varen_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: vote-airswap.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\vote-airswap_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: vote-vitadao.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\vote-vitadao_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: vsp.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\vsp_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: xdaistake.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\xdaistake_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: yam.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\yam_3_consecutive_bundle_turnover_ratio.csv\n",
            "Processed file: ybaby.eth_merged_data.csv, 3-Consecutive Bundle Turnover Ratios saved to: D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\\ybaby_3_consecutive_bundle_turnover_ratio.csv\n"
          ]
        }
      ],
      "source": [
        "# 3 consecutive proposals as bundle, calculate turnover rate\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the directory containing the files\n",
        "directory_path = r\"D:\\DAO_Voting_1101\\merged_data_files\\FT\"\n",
        "\n",
        "# Output directory for the new calculation\n",
        "output_directory = r\"D:\\DAO_Voting_1101\\Statistics\\voter_stats\\3_consecutive_bundle_turnover_ratio\"\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Iterate through each file in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        # Read the CSV file\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)\n",
        "\n",
        "        # Sort DataFrame by 'end' column\n",
        "        df.sort_values(by='end', inplace=True)\n",
        "\n",
        "        # Turnover Rate Calculation for 3-consecutive proposal bundles\n",
        "        proposals = df['proposal_id'].unique()\n",
        "\n",
        "        turnover_ratios = {}\n",
        "        # Start from the first full bundle\n",
        "        for i in range(3, len(proposals)):\n",
        "            # Define the bundles\n",
        "            current_bundle = proposals[i-2:i+1]\n",
        "            previous_bundle = proposals[i-3:i]\n",
        "\n",
        "            # Collect voters for each bundle\n",
        "            current_voters = set(df[df['proposal_id'].isin(current_bundle)]['voter_address'])\n",
        "            previous_voters = set(df[df['proposal_id'].isin(previous_bundle)]['voter_address'])\n",
        "\n",
        "            # Calculate intersection and union of voters\n",
        "            intersection_count = len(current_voters.intersection(previous_voters))\n",
        "            union_count = len(current_voters.union(previous_voters))\n",
        "\n",
        "            turnover_ratio = intersection_count / union_count if union_count > 0 else 0\n",
        "            turnover_ratios[f'{previous_bundle} -> {current_bundle}'] = turnover_ratio\n",
        "            \n",
        "        # Create DataFrame from turnover_ratios\n",
        "        result_df = pd.DataFrame(list(turnover_ratios.items()), columns=['proposal_bundle_pair','turnover_ratio'])\n",
        "\n",
        "        # Extract protocol_name from the filename\n",
        "        protocol_name = filename.split(\"_\")[0].split(\".\")[0]\n",
        "\n",
        "        # Save the DataFrame to CSV\n",
        "        output_filename = f'{protocol_name}_3_consecutive_bundle_turnover_ratio.csv'\n",
        "        output_path = os.path.join(output_directory, output_filename)\n",
        "        result_df.to_csv(output_path, index=False)\n",
        "\n",
        "        print(f\"Processed file: {filename}, 3-Consecutive Bundle Turnover Ratios saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "folder_path = r\"D:\\DAO_Voting_1101\\Statistics\\voter_stats\\consecutive_two_turnover_ratio\"\n",
        "\n",
        "# Iterate over all CSV files in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        # Load the CSV file into a DataFrame\n",
        "        df = pd.read_csv(file_path)\n",
        "        # Split the 'proposal_pair' column on the arrow symbol\n",
        "        df['second_proposal'] = df['proposal_pair'].apply(lambda x: x.split(' -> ')[1])\n",
        "        # Save the DataFrame back to the original file\n",
        "        df.to_csv(file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Turnover Ratios saved to the output directory.\n"
          ]
        }
      ],
      "source": [
        "# over 15 per voter turnover ratio \n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the directory containing the files\n",
        "directory_path = r\"D:\\DAO_Voting_1101\\merged_data_files\\FT\"\n",
        "\n",
        "# Output directory\n",
        "output_directory = r\"D:\\DAO_Voting_1101\\Statistics\\voter_stats\\consecutive_two_turnover_ratio\\over15per_voter_turnover_ratio\"\n",
        "\n",
        "# Ensure the output directory exists\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "# Iterate through each file in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        # Read the CSV file\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)\n",
        "\n",
        "        # Sort DataFrame by 'end' column\n",
        "        df.sort_values(by='end', inplace=True)\n",
        "\n",
        "        # Filter voters with more than 15% accountability\n",
        "        df_filtered = df[df['accountability'] > 0.15]\n",
        "\n",
        "        # Turnover Rate Calculation\n",
        "        proposals = df_filtered['proposal_id'].unique()\n",
        "\n",
        "        turnover_ratios = {}\n",
        "        for i in range(1, len(proposals)):\n",
        "            current_proposal = proposals[i]\n",
        "            previous_proposal = proposals[i - 1]\n",
        "\n",
        "            current_voters = set(df_filtered[df_filtered['proposal_id'] == current_proposal]['voter_address'])\n",
        "            previous_voters = set(df_filtered[df_filtered['proposal_id'] == previous_proposal]['voter_address'])\n",
        "\n",
        "            intersection_count = len(current_voters.intersection(previous_voters))\n",
        "            union_count = len(current_voters.union(previous_voters))\n",
        "\n",
        "            turnover_ratio = intersection_count / union_count if union_count != 0 else 0\n",
        "            turnover_ratios[f'{previous_proposal} -> {current_proposal}'] = turnover_ratio\n",
        "            \n",
        "        # Create DataFrame from turnover_ratios\n",
        "        result_df = pd.DataFrame(list(turnover_ratios.items()), columns=['proposal_pair','turnover_ratio'])\n",
        "\n",
        "        # Extract protocol_name from the filename\n",
        "        protocol_name = filename.split(\"_\")[0].split(\".\")[0]\n",
        "\n",
        "        # Save the DataFrame to CSV\n",
        "        output_filename = f'{protocol_name}_over15per_voter_turnover_ratio.csv'\n",
        "        output_path = os.path.join(output_directory, output_filename)\n",
        "        result_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"Turnover Ratios saved to the output directory.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "folder_path = r'D:\\DAO_Voting_1101\\Statistics\\voter_stats\\consecutive_two_turnover_ratio\\over15per_voter_turnover_ratio'\n",
        "\n",
        "# Iterate over all CSV files in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        # Load the CSV file into a DataFrame\n",
        "        df = pd.read_csv(file_path)\n",
        "        # Split the 'proposal_pair' column on the arrow symbol\n",
        "        df['second_proposal'] = df['proposal_pair'].apply(lambda x: x.split(' -> ')[1])\n",
        "        # Save the DataFrame back to the original file\n",
        "        df.to_csv(file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All files have the ideal column list.\n"
          ]
        }
      ],
      "source": [
        "## check the column list\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the directory path\n",
        "directory_path = r'C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\merged_data_files\\FT'\n",
        "\n",
        "# Define the ideal column list\n",
        "ideal_columns = [\n",
        "    'proposal_id', 'voter_address', 'choice', 'voting_power', 'timestamp', 'title',\n",
        "    'author', 'votes', 'type', 'choices', 'scores', 'Num_choices', 'Win_choice',\n",
        "    'Win_score', 'Winrate', 'scores_total', 'discussion', 'link', 'quorum', 'start',\n",
        "    'start_real_time', 'end', 'end_real_time', 'state', 'blockNumber',\n",
        "    'relative_time_voting', 'win_option_index'\n",
        "]\n",
        "\n",
        "# Function to check if a DataFrame has the ideal columns\n",
        "def has_ideal_columns(file_path):\n",
        "    df = pd.read_csv(file_path, nrows=0)  # Read only the header\n",
        "    return all(column in df.columns for column in ideal_columns)\n",
        "\n",
        "# Check each file in the directory\n",
        "non_fitted_files = [\n",
        "    file for file in os.listdir(directory_path) if file.endswith('.csv') and not has_ideal_columns(os.path.join(directory_path, file))\n",
        "]\n",
        "\n",
        "# Print the non-fitted file names\n",
        "if non_fitted_files:\n",
        "    print(\"Non-fitted files:\")\n",
        "    for file in non_fitted_files:\n",
        "        print(file)\n",
        "else:\n",
        "    print(\"All files have the ideal column list.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6BP-oNSsPEq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Specify the directory containing the files\n",
        "directory_path = r'C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\merged_data_files\\FT'\n",
        "\n",
        "# Output directory\n",
        "output_directory = r\"C:\\Users\\ChunghyunHan\\Desktop\\DAO_Voting_1101\\Statistics\\voter_stats\\avgRTV&top1_5_10&whale_votes_diff\"\n",
        "\n",
        "# Iterate through each file in the directory\n",
        "for filename in os.listdir(directory_path)[2:]:\n",
        "    try:\n",
        "        if filename.endswith(\".csv\"):\n",
        "            # Read the CSV file\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)\n",
        "            print(file_path)\n",
        "\n",
        "            # Initialize an empty list to store the statistics for each proposal\n",
        "            stats_list = []\n",
        "\n",
        "            for proposal_id, group_df in df.groupby('proposal_id'):\n",
        "\n",
        "                try:\n",
        "                    # Calculate the specified statistics for each proposal\n",
        "                    df['relative_time_voting'] = df['relative_time_voting'].astype(float)\n",
        "                    \n",
        "                    avg_RTV = group_df['relative_time_voting'].mean()\n",
        "                    std_RTV = group_df['relative_time_voting'].std()\n",
        "\n",
        "                    top_1_voter_address = group_df.loc[group_df['voting_power'].idxmax(), 'voter_address']\n",
        "                    top_5_voters_address = group_df.nlargest(5, 'voting_power')['voter_address'].tolist()\n",
        "                    top_10_voters_address = group_df.nlargest(10, 'voting_power')['voter_address'].tolist()\n",
        "\n",
        "                    top_1_voter_VP = group_df.loc[group_df['voting_power'].idxmax(), 'voting_power']\n",
        "                    top_5_voters_VP = group_df.nlargest(5, 'voting_power')['voting_power'].tolist()\n",
        "                    top_10_voters_VP = group_df.nlargest(10, 'voting_power')['voting_power'].tolist()\n",
        "                    \n",
        "                    total_voting_power = group_df['voting_power'].sum()\n",
        "                    top_1_voter_accountability = top_1_voter_VP / total_voting_power\n",
        "                    top_5_voters_accountability = sum(top_5_voters_VP) / total_voting_power\n",
        "                    top_10_voters_accountability = sum(top_10_voters_VP) / total_voting_power\n",
        "\n",
        "                    top_1_voter_RTV = group_df.loc[group_df['voting_power'].idxmax(), 'relative_time_voting']\n",
        "                    top_5_voters_RTV = group_df.nlargest(5, 'voting_power')['relative_time_voting'].tolist()\n",
        "                    top_10_voters_RTV = group_df.nlargest(10, 'voting_power')['relative_time_voting'].tolist()\n",
        "\n",
        "                    choice_top_1 = group_df.loc[group_df['voting_power'].idxmax(), 'choice']\n",
        "                    choices_top_5 = group_df.nlargest(5, 'voting_power')['choice'].tolist()\n",
        "                    choices_top_10 = group_df.nlargest(10, 'voting_power')['choice'].tolist()\n",
        "\n",
        "                    choices_dict_top_5 = dict(zip(range(1, 6), choices_top_5))\n",
        "                    choices_dict_top_10 = dict(zip(range(1, 11), choices_top_10))\n",
        "\n",
        "                    # Calculate relative time voting average of voters other than top 10\n",
        "                    other_voters_RTV = group_df[~group_df['voter_address'].isin(top_10_voters_address)]['relative_time_voting']\n",
        "                    avg_other_voters_RTV = other_voters_RTV.mean()\n",
        "\n",
        "                    choices = group_df['choices']\n",
        "                    win_choice_index = group_df['win_option_index'].iloc[0]\n",
        "                    \n",
        "                    top1_winning_opt_difference = 1 if choice_top_1 != win_choice_index else 0\n",
        "\n",
        "                    choices_identical_top_1_5 = {index: 0 if choices_dict_top_5.get(index, -1) == choice_top_1 else 1 for index in choices_dict_top_5}\n",
        "                    choices_identical_top_1_10 = {index: 0 if choices_dict_top_10.get(index, -1) == choice_top_1 else 1 for index in choices_dict_top_10}\n",
        "\n",
        "                    num_diff_top_1_5 = sum(choices_identical_top_1_5.values())\n",
        "                    num_diff_top_1_10 = sum(choices_identical_top_1_10.values())\n",
        "\n",
        "                    # Append the results to the list\n",
        "                    result = {\n",
        "                        'proposal_id': proposal_id,\n",
        "                        'avg_RTV': avg_RTV,\n",
        "                        'std_RTV': std_RTV,\n",
        "                        'top_1_voter_address': top_1_voter_address,\n",
        "                        'top_5_voters_address': top_5_voters_address,\n",
        "                        'top_10_voters_address': top_10_voters_address,\n",
        "                        'top_1_voter_RTV': top_1_voter_RTV,\n",
        "                        'top_5_voters_RTV': top_5_voters_RTV,\n",
        "                        'top_10_voters_RTV': top_10_voters_RTV,\n",
        "                        'avg_other_voters_RTV': avg_other_voters_RTV,\n",
        "                        'top_1_voter_accountability': top_1_voter_accountability,\n",
        "                        'top_5_voters_accountability': top_5_voters_accountability,\n",
        "                        'top_10_voters_accountability': top_10_voters_accountability,\n",
        "                        'win_option_index' : win_choice_index,\n",
        "                        'choice_top_1': choice_top_1,\n",
        "                        'top1_win_option_diff': top1_winning_opt_difference,\n",
        "                        'choice_top_5': choices_top_5,\n",
        "                        'choice_top_10': choices_top_10,\n",
        "                        'choices_identical_top_1_5': choices_identical_top_1_5,\n",
        "                        'choices_identical_top_1_10': choices_identical_top_1_10,\n",
        "                        'num_diff_top_1_5': num_diff_top_1_5,\n",
        "                        'num_diff_top_1_10': num_diff_top_1_10\n",
        "                    }\n",
        "\n",
        "                    stats_list.append(result)\n",
        "                except Exception as inner_error:\n",
        "                    print(f\"Error processing proposal_id {proposal_id} in file {filename}\\nError: {str(inner_error)}\")\n",
        "\n",
        "            # Create a DataFrame from the list of statistics\n",
        "            stats_df = pd.DataFrame(stats_list)\n",
        "\n",
        "            # Extract protocol_name from the filename\n",
        "            protocol_name = filename.split(\"_\")[0].split(\".\")[0]\n",
        "\n",
        "            # Save the DataFrame to CSV\n",
        "            output_filename = f'{protocol_name}_top1_5_10_stats.csv'\n",
        "            output_path = os.path.join(output_directory, output_filename)\n",
        "            stats_df.to_csv(output_path, index=False)\n",
        "\n",
        "            print(f\"Processed file: {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {filename}\\nError: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGqcNuSzsPEq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# holding and voting\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJKeaqppsPEq"
      },
      "outputs": [],
      "source": [
        "# (8) voters_holders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the directory path\n",
        "directory_path = r\"D:\\DAO_Voting_1101\\merged_data_files\\FT\"\n",
        "\n",
        "\n",
        "\n",
        "# Iterate through each CSV file in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        # Read the CSV file\n",
        "        protocol_name = filename.split(\"_\")[0].split(\".\")[0]\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        \n",
        "        df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)\n",
        "\n",
        "        # Identify voters who participated in more than one proposal\n",
        "        multiple_participations = df.groupby('voter_address')['proposal_id'].nunique()\n",
        "        multiple_participations = multiple_participations[multiple_participations > 1].index.tolist()\n",
        "\n",
        "        # Filter the original DataFrame for voters with multiple participations\n",
        "        filtered_df = df[df['voter_address'].isin(multiple_participations)]\n",
        "\n",
        "        # Add a new column for protocol name in the first position\n",
        "        filtered_df.insert(0, 'protocol_name', protocol_name)\n",
        "\n",
        "        # Specify specific columns from the original DataFrame to include\n",
        "        specific_columns = ['protocol_name', 'proposal_id', 'voter_address', 'choice', 'voting_power', 'author', 'choices', 'Win_choice', 'Winrate', 'start', 'end' ]\n",
        "\n",
        "        # Concatenate the filtered DataFrame to the result DataFrame with specific columns\n",
        "        result_df = pd.concat([result_df, filtered_df[specific_columns]])\n",
        "\n",
        "    # Save the result DataFrame to a CSV file with protocol name in the filename\n",
        "    result_df.to_csv(r\"D:\\DAO_Voting_1101\\Statistics\\voter_stats\\multiple_times_voters\\voting_results\\{}_multiple_participations_voting_result.csv\".format(protocol_name), index=False)\n",
        "    # Create an empty DataFrame to store the results\n",
        "    result_df = pd.DataFrame()\n",
        "\n",
        "    print(f\"Processed file: {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "# 1. multiple voters stat by protocol\n",
        "## num max, min, avg, std of voters \n",
        "\n",
        "# Define the directory path\n",
        "directory_path = r\"D:\\DAO_Voting_1101\\Statistics\\voter_stats\\multiple_times_voters\\voting_results\"\n",
        "\n",
        "# Empty list to store statistics for each file\n",
        "stats_list = []\n",
        "\n",
        "# Iterate through each CSV file in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        # Read the CSV file\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)\n",
        "\n",
        "        # Group by 'voter_address' and aggregate the count and list of unique 'proposal_id'\n",
        "        voter_participation_info = df.groupby('voter_address').agg({\n",
        "            'proposal_id': ['count', lambda x: x.unique().tolist()],\n",
        "            'author': lambda x: x.tolist(),\n",
        "            'choice': lambda x: x.tolist()\n",
        "        }).reset_index()\n",
        "\n",
        "        # Add a new column 'unique_authors' with lists of unique authors\n",
        "        voter_participation_info['unique_authors'] = df.groupby('voter_address')['author'].unique().apply(list).reset_index()['author']\n",
        "        voter_participation_info['num_unique_authors'] = voter_participation_info['unique_authors'].apply(len)\n",
        "\n",
        "        # Rename the columns for clarity\n",
        "        voter_participation_info.columns = ['voter_address', 'num_proposals', 'proposal_ids', 'author', 'choice', 'unique_authors', 'num_unique_authors']\n",
        "\n",
        "        # Convert the proposal_ids, author, and choice columns to strings for better representation\n",
        "        voter_participation_info['proposal_ids'] = voter_participation_info['proposal_ids'].apply(list)\n",
        "        voter_participation_info['author'] = voter_participation_info['author'].apply(list)\n",
        "        voter_participation_info['choice'] = voter_participation_info['choice'].apply(list)\n",
        "        voter_participation_info['unique_authors'] = voter_participation_info['unique_authors'].apply(list)\n",
        "\n",
        "        # Export the result DataFrame to a CSV file\n",
        "        protocol_name = filename.split(\"_\")[0]\n",
        "        voter_participation_info.to_csv(r\"D:\\DAO_Voting_1101\\Statistics\\voter_stats\\multiple_times_voters\\{}_multiple_time_voters.csv\".format(protocol_name), index=False)\n",
        "\n",
        "        # Update the stats list with information about the exported file\n",
        "        stats_list.append({\n",
        "            'protocol_name': protocol_name,\n",
        "            'num_unique_multiple_voters': len(voter_participation_info),\n",
        "            'num_proposals_min': voter_participation_info['num_proposals'].min(),\n",
        "            'num_proposals_max': voter_participation_info['num_proposals'].max(),\n",
        "            'num_proposals_avg': voter_participation_info['num_proposals'].mean(),\n",
        "            'num_proposals_std': voter_participation_info['num_proposals'].std()\n",
        "        })\n",
        "\n",
        "        print(f\"Processed file: {filename}\")\n",
        "\n",
        "# Create a DataFrame with summary statistics\n",
        "summary_stats_df = pd.DataFrame(stats_list)\n",
        "\n",
        "# Save the summary statistics DataFrame to a CSV file\n",
        "summary_stats_df.to_csv(r\"D:\\DAO_Voting_1101\\Statistics\\voter_stats\\multiple_times_voters\\multiple_voters_summary_statistics.csv\", index=False)\n",
        "\n",
        "# Display the summary statistics DataFrame\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(summary_stats_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def exploding( file_address ):\n",
        "    df  = pd.read_csv(file_address)\n",
        "    df['proposal_ids'] = df['proposal_ids'].map(lambda x: eval(x))\n",
        "    df['author'] = df['author'].map(lambda x: eval(x))\n",
        "    df['choice'] = df['choice'].map(lambda x: eval(x))\n",
        "    df['pac'] = df.apply(lambda x:list(zip(*[x['proposal_ids'], x['author'], x['choice']])) , axis = 1)\n",
        "\n",
        "    df_e = df.explode('pac').reset_index(drop = True)\n",
        "\n",
        "    df_e['proposal_ids'] = df_e['pac'].map(lambda x: x[0])\n",
        "    df_e['author'] = df_e['pac'].map(lambda x: x[1])\n",
        "    df_e['choice'] = df_e['pac'].map(lambda x: x[2])\n",
        "    \n",
        "    return df_e\n",
        "\n",
        "def find_reciprocal_relationship(df_e):\n",
        "    df_c = df_e[(df_e['voter_address'].isin(df_e['author']) & df_e['voter_address'].isin(df_e['author']))] \n",
        "\n",
        "    df_c_condition1 = df_c[['voter_address','author']].apply(lambda x: str(sorted(x)), axis=1)\n",
        "    df_c_condition2 = df_c[['voter_address','author']].apply(lambda x: str(sorted(x)), axis=1).value_counts()\n",
        "\n",
        "    df_c2 = df_c[df_c_condition1.isin(df_c_condition2[df_c_condition2>1].index)].copy()\n",
        "\n",
        "    df_c2['va_sorted'] = df_c2[['voter_address','author']].apply(lambda x: str(sorted(x)), axis=1)\n",
        "    df_c2['same_as_sorted'] = df_c2['va_sorted'] == df_c2[['voter_address','author']].apply(lambda x: str([x['voter_address'],x['author']]),axis=1)\n",
        "\n",
        "    reciprocal_ids = df_c2.groupby(['va_sorted'])['same_as_sorted'].unique()[df_c2.groupby(['va_sorted'])['same_as_sorted'].unique().map(len) > 1]\n",
        "    \n",
        "    relationship = df_c2[df_c2['va_sorted'].isin(reciprocal_ids.index)]\n",
        "\n",
        "    return relationship\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the directory path\n",
        "directory_path = r\"D:\\DAO_Voting_1101\\Statistics\\voter_stats\\multiple_times_voters\"\n",
        "\n",
        "# Output directory for reciprocal relationships\n",
        "output_directory = r\"D:\\DAO_Voting_1101\\Statistics\\voter_stats\\reciprocity\"\n",
        "\n",
        "# Iterate through each CSV file in the directory\n",
        "for filename in sorted(os.listdir(directory_path)):\n",
        "    \n",
        "    if filename.endswith(\".csv\"):\n",
        "        # Read the CSV file\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "        df_e = exploding(file_path)\n",
        "\n",
        "        relationship = find_reciprocal_relationship(df_e)\n",
        "        relationship_drop = relationship.drop(['unique_authors', 'num_unique_authors', 'pac', 'va_sorted', 'same_as_sorted'], axis=1)\n",
        "        relationship_drop.to_csv(os.path.join(output_directory, f\"{filename.split('.')[0].split('_')[0]}_reciprocal_relationships.csv\"), index=False)\n",
        "\n",
        "    print(f\"Finished processing {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Empty list to store statistics for each file\n",
        "stats_list = []\n",
        "\n",
        "directory_path = r\"D:\\DAO_Voting_1101\\Statistics\\voter_stats\\reciprocity\"\n",
        "# Iterate through each CSV file in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        # Read the CSV file\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        df = pd.read_csv(file_path, encoding='ISO-8859-1', low_memory=False)\n",
        "\n",
        "        # Export the result DataFrame to a CSV file\n",
        "        protocol_name = filename.split(\"_\")[0]\n",
        "\n",
        "        # Update the stats list with information about the exported file\n",
        "        stats_list.append({\n",
        "            'protocol_name': protocol_name,\n",
        "            'num_reciprocal_proposals': len(df),\n",
        "            'num_unique_authors': df['author'].nunique(),\n",
        "            'num_unique_voters': df['voter_address'].nunique()\n",
        "        })\n",
        "    print(f\"Processed file: {filename}\")\n",
        "\n",
        "# Create a DataFrame with summary statistics\n",
        "summary_stats_df = pd.DataFrame(stats_list)\n",
        "\n",
        "# Save the summary statistics DataFrame to a CSV file\n",
        "summary_stats_df.to_csv(r\"D:\\DAO_Voting_1101\\Statistics\\voter_stats\\reciprocal_summary_statistics.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reciprocal voting 회수 : len\n",
        "\n",
        "# unique author 수, unique voter 수 \n",
        "\n",
        "\n",
        "# unique voter 중 whale voter 수\n",
        "# proposal 별 whale voter list : {protocol}_top1_5_10_stats\n",
        "    # 그 voter의 그 프로포절에서의 VP\n",
        "    # 그 proposal의 토탈 voter 수, VP -> 그 가운데 해당 voter의 VP와 비중, 등수\n",
        "\n",
        "\n",
        "# unique author 중 whale voter의 수 \n",
        "\n",
        "## step 1. merged_data에서 해당 프로포절을 찾아서, 그 voter의 등수 찾기\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def extract_proposals_and_rank(merged_data_path, reciprocal_path):\n",
        "    # Read data from file 1\n",
        "    df1 = pd.read_csv(merged_data_path, encoding='ISO-8859-1', low_memory=False)\n",
        "\n",
        "    # Read data from file 2\n",
        "    df2 = pd.read_csv(reciprocal_path)\n",
        "\n",
        "    # Extract unique proposal_ids from file \n",
        "    unique_proposals = df2['proposal_ids'].unique()\n",
        "\n",
        "    extracted_proposals = df1[df1['proposal_id'].isin(unique_proposals)].copy()\n",
        "\n",
        "    # Calculate the rank of voters by 'voting_power' within each 'proposal_id' group\n",
        "    extracted_proposals.loc[:, 'rank'] = extracted_proposals.groupby('proposal_id')['voting_power'].rank(ascending=False, method='min')\n",
        "\n",
        "    return extracted_proposals\n",
        "\n",
        "\n",
        "def extract_matching_voters(merged_data_path, reciprocal_path):\n",
        "    # Read data from file 1\n",
        "    extracted_file = extract_proposals_and_rank(merged_data_path, reciprocal_path)\n",
        "\n",
        "    # Read data from file 2\n",
        "    df2 = pd.read_csv(reciprocal_path)\n",
        "\n",
        "    # Extract unique voter addresses from file 1\n",
        "    unique_voters = df2['voter_address'].unique()\n",
        "    \n",
        "    # Extract rows from file 1 where 'voter_address' matches those in file 2\n",
        "    matching_rows = extracted_file[extracted_file['voter_address'].isin(unique_voters)].copy()\n",
        "\n",
        "    # Avoid SettingWithCopyWarning by using .loc to assign values\n",
        "    matching_rows.loc[:, 'accountability'] = matching_rows['voting_power'] / matching_rows['scores_total']\n",
        "    \n",
        "    return matching_rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# unique voter 중 whale voter 수\n",
        "# proposal 별 whale voter list : {protocol}_top1_5_10_stats\n",
        "    # 그 voter의 그 프로포절에서의 VP\n",
        "    # 그 proposal의 토탈 voter 수, VP -> 그 가운데 해당 voter의 VP와 비중, 등수\n",
        "\n",
        "merged_data_path = r'D:\\DAO_Voting_1101\\merged_data_files\\FT'\n",
        "reciprocal_path = r'D:\\DAO_Voting_1101\\Statistics\\voter_stats\\reciprocity'\n",
        "output_directory = r\"D:\\DAO_Voting_1101\\Statistics\\voter_stats\\reciprocity\\reciprocal_voting_data\"\n",
        "\n",
        "for filename in sorted(os.listdir(merged_data_path)):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        m_protocol_name = filename.split(\"_\")[0].split(\".\")[0]\n",
        "        \n",
        "        m_path = os.path.join(merged_data_path, filename)\n",
        "\n",
        "        for r_filename in sorted(os.listdir(reciprocal_path)):\n",
        "\n",
        "            if r_filename.split(\"_\")[0] == m_protocol_name:\n",
        "                \n",
        "                r_path = os.path.join(reciprocal_path, r_filename)\n",
        "                result = extract_matching_voters(m_path, r_path)\n",
        "                result = result.drop(['title','votes','type','scores','Num_choices','Winrate','discussion','link','quorum','start','start_real_time','end','end_real_time','state','blockNumber'], axis=1)\n",
        "                result.to_csv(os.path.join(output_directory, f\"{m_protocol_name}_reciprocal_voting_data.csv\"), index=False)\n",
        "\n",
        "    print(f\"Finished processing {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Reciprocity Stats\n",
        "# proposal 개수 -> row 수 \n",
        "\n",
        "# unique author 수, unique voter 수 \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
